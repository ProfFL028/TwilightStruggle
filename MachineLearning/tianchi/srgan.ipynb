{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SRGAN:\n",
    "    def __init__(self):\n",
    "        self.gen = self.generator()\n",
    "        self.dis = self.discriminator()\n",
    "        self.vgg = self.vgg19()\n",
    "        self.model_generator = self.build_generator(self.gen, self.dis, self.vgg)\n",
    "\n",
    "    def generator(self):\n",
    "        inputs = keras.layers.Input(shape=(270, 480, 3))\n",
    "        cnn = keras.layers.Conv2D(64, 9, padding='same', activation='relu')(inputs)\n",
    "\n",
    "        cnn = keras.layers.Conv2D(64, 3, padding='same')(cnn)\n",
    "        cnn = keras.layers.BatchNormalization()(cnn)\n",
    "        cnn = tf.nn.relu(cnn)\n",
    "\n",
    "        cnn_first = cnn\n",
    "        for index in range(16):\n",
    "            cnn_ori = cnn\n",
    "            cnn = keras.layers.Conv2D(64, 3, padding='same')(cnn)\n",
    "            cnn = keras.layers.BatchNormalization()(cnn)\n",
    "            cnn = tf.nn.relu(cnn)\n",
    "\n",
    "            cnn = keras.layers.Conv2D(64, 3, padding='same')(cnn)\n",
    "            cnn = keras.layers.BatchNormalization()(cnn)\n",
    "            cnn = cnn + cnn_ori\n",
    "\n",
    "        cnn = keras.layers.Conv2D(64, 3, padding='same')(cnn)\n",
    "        cnn = keras.layers.BatchNormalization()(cnn)\n",
    "        cnn = cnn_first + cnn\n",
    "\n",
    "        for index in range(2):\n",
    "            cnn = keras.layers.Conv2D(1, 3, padding='same')(cnn)\n",
    "            cnn = keras.layers.UpSampling2D(size=2)(cnn)\n",
    "            cnn = tf.nn.relu(cnn)\n",
    "\n",
    "        cnn = keras.layers.Conv2D(3, 9, padding='same')(cnn)\n",
    "        outputs = tf.nn.tanh(cnn)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "        model.compile(optimizer=tf.optimizers.Adam(1e-1), loss='mse', metrics=['mse'])\n",
    "        return model\n",
    "\n",
    "    def discriminator(self):\n",
    "        inputs = keras.layers.Input(shape=(270 * 4, 480 * 4, 3))\n",
    "\n",
    "        cnn = keras.layers.Conv2D(64, 3, strides=1, padding='same')(inputs)\n",
    "        cnn = tf.nn.relu(cnn)\n",
    "\n",
    "        cnn = keras.layers.Conv2D(64, 3, strides=2, padding='same')(cnn)\n",
    "        cnn = keras.layers.BatchNormalization()(cnn)\n",
    "        cnn = tf.nn.relu(cnn)\n",
    "\n",
    "        for filters in [128, 256, 512]:\n",
    "            for strides in [1, 2]:\n",
    "                cnn = keras.layers.Conv2D(filters, 3, strides=strides, padding='same')(cnn)\n",
    "                cnn = keras.layers.BatchNormalization()(cnn)\n",
    "                cnn = tf.nn.relu(cnn)\n",
    "        cnn = keras.layers.Flatten()(cnn)\n",
    "        dense = keras.layers.Dense(1024)(cnn)\n",
    "        dense = tf.nn.relu(dense)\n",
    "        dense = keras.layers.Dense(1)(dense)\n",
    "\n",
    "        outputs = tf.nn.sigmoid(dense)\n",
    "\n",
    "        model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "        model.compile(optimizer=tf.optimizers.Adam(1e-1, loss='binary_crossentropy', metrics=['accuracy']))\n",
    "        return model\n",
    "\n",
    "    def vgg19(self):\n",
    "        vgg19 = keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(270 * 4, 480 * 4, 3))\n",
    "        vgg19.trainable = False\n",
    "        for l in vgg19.layers:\n",
    "            l.trainable = False\n",
    "        model = keras.Model(inputs=vgg19.input, outputs=vgg19.get_layer('block5_conv4').output)\n",
    "        model.compile(optimizer=tf.optimizers.Adam(1e-1), loss='mse', metrics=[\"mse\"])\n",
    "        return model\n",
    "\n",
    "    def build_generator(self, gen, dis, vgg):\n",
    "        discriminator = dis\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        vgg = vgg\n",
    "        vgg.trainable = False\n",
    "\n",
    "        generator = gen\n",
    "        discriminator_prob = discriminator(generator.outputs)\n",
    "        vgg_features = vgg(generator.outputs)\n",
    "\n",
    "        model = keras.Model(inputs=[generator.inputs], outputs=[generator.outputs, vgg_features, discriminator_prob])\n",
    "\n",
    "        model.compile(optimizer=tf.optimizers.Adam(1e-1), loss=['mse', 'mse', 'binary_crossentropy'],\n",
    "                      loss_weights=[1., 2e-6, 1e-3])\n",
    "        return model\n",
    "\n",
    "    def model_train(self, img_l, img_h):\n",
    "        batch_size = img_l.shape[0]\n",
    "        label_real = np.array([[0]] * batch_size)\n",
    "        label_fake = np.array([[1]] * batch_size)\n",
    "        self.dis.trainable = False\n",
    "        self.vgg.trainable = False\n",
    "\n",
    "        img_GT_vgg_features = self.vgg.predict(img_h)\n",
    "\n",
    "        self.model_generator.train_on_batch(img_l, [img_h, img_GT_vgg_features, label_real])\n",
    "\n",
    "        self.dis.trainable = True\n",
    "\n",
    "        img_pred = self.model_generator.predict(img_l)\n",
    "        self.dis.train_on_batch(img_pred, label_fake)\n",
    "        self.dis.train_on_batch(img_h, label_real)\n",
    "\n",
    "    def model_pred(self, img_l):\n",
    "        img_super, vgg_features, prob_fake_or_real = self.model_generator.predict(img_l)\n",
    "        return img_super\n",
    "\n",
    "    def model_evaluate(self, img_l, img_GT):\n",
    "        batch_size = img_l.shape[0]\n",
    "        label_real = np.array([[0]] * batch_size)\n",
    "        img_GT_vgg_features = self.vgg.predict(img_GT)\n",
    "        generator_loss, vgg_features_content_loss, discriminator_loss = self.model_generator.evaluate(img_l, [img_GT,\n",
    "                                                                                                              img_GT_vgg_features,\n",
    "                                                                                                              label_real])\n",
    "        return generator_loss, vgg_features_content_loss, discriminator_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[4177920,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-1e986d3faac3>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0msrgan\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSRGAN\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mimg_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'youku'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'IMG_LOW'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Youku_00000_l'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'034.bmp'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mimg_l\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mimg_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'youku'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'IMG_HIGH'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'Youku_00000_h_GT'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'034.bmp'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-e8e448180536>\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgen\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgenerator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdis\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdiscriminator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvgg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvgg19\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_generator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild_generator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdis\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvgg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-e8e448180536>\u001B[0m in \u001B[0;36mdiscriminator\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     57\u001B[0m                 \u001B[0mcnn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcnn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m         \u001B[0mcnn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFlatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcnn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 59\u001B[1;33m         \u001B[0mdense\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1024\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcnn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     60\u001B[0m         \u001B[0mdense\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     61\u001B[0m         \u001B[0mdense\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdense\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     68\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\keras\\backend.py\u001B[0m in \u001B[0;36mrandom_uniform\u001B[1;34m(self, shape, minval, maxval, dtype)\u001B[0m\n\u001B[0;32m   1829\u001B[0m       return self._generator.uniform(\n\u001B[0;32m   1830\u001B[0m           shape=shape, minval=minval, maxval=maxval, dtype=dtype)\n\u001B[1;32m-> 1831\u001B[1;33m     return tf.random.uniform(\n\u001B[0m\u001B[0;32m   1832\u001B[0m         \u001B[0mshape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mminval\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mminval\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxval\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmaxval\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1833\u001B[0m         seed=self.make_legacy_seed())\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: OOM when allocating tensor with shape[4177920,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:Mul]"
     ]
    }
   ],
   "source": [
    "srgan = SRGAN()\n",
    "import os\n",
    "img_path = os.path.join('youku', 'IMG_LOW', 'Youku_00000_l', '034.bmp')\n",
    "img_l = cv2.imread(img_path)\n",
    "img_path = os.path.join('youku', 'IMG_HIGH', 'Youku_00000_h_GT', '034.bmp')\n",
    "img_h = cv2.imread(img_path)\n",
    "x = np.array([img_l, img_l])\n",
    "y = np.array([img_h, img_h])\n",
    "srgan.model_train(x, y)\n",
    "srgan.model_evaluate(x, y)\n",
    "pic_super = srgan.model_pred(x)\n",
    "cv2.imwrite(\"youku/GEN/srgan_00.bmp\", pic_super[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}