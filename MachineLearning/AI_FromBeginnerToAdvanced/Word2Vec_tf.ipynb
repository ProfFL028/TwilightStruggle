{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3实战：动手写Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实验说明：该代码实现的应用比较简单，利用Tensorflow2.x框架搭建Skip-gram模型以实现文本向量化，并计算输出与测试词最相近的10个词。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入依赖模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 读取停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载停用词表，去掉停用词中的回车、换行符、空格，打印停用词的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1910\n",
      "1909\n"
     ]
    }
   ],
   "source": [
    "stop_words=[]\n",
    "with open(\"/home/Word2VecTest/data/stop_words.txt\", \"r\", encoding=\"utf-8\") as f_stopwords:\n",
    "    for line in f_stopwords:\n",
    "        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "        stop_words.append(line)\n",
    "print(len(stop_words))\n",
    "stop_words=set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 文本预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####     首先定义一个列表用于存储分完词的文本列表，然后读取文本数据集，去掉文本中的回车、换行符、空格，接着进行分词处理，并对分词后结果进行停用词过滤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.778 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272481\n",
      "41043\n"
     ]
    }
   ],
   "source": [
    "raw_word_list = []\n",
    "rules = u\"([\\u4e00-\\u9fa5]+)\"\n",
    "pattern = re.compile(rules)\n",
    "f_writer = open(\"/home/Word2VecTest/data/Seg_The_Smiling_Proud_Wanderer.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"/home/Word2VecTest/data//The_Smiling_Proud_Wanderer.txt\", \"r\", encoding=\"utf-8\") as f_reader:\n",
    "    lines = f_reader.readlines()\n",
    "    for line in lines:\n",
    "        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "        if line == \"\" or line is None:\n",
    "            continue\n",
    "        line = \" \".join(jieba.cut(line))\n",
    "        seg_list = pattern.findall(line)\n",
    "        word_list = []\n",
    "        for word in seg_list:\n",
    "            if word not in stop_words:\n",
    "                word_list.append(word)\n",
    "        if len(word_list) > 0:\n",
    "            raw_word_list.extend(word_list)\n",
    "            line = \" \".join(word_list)\n",
    "            # line=\" \".join(seg_list)\n",
    "            f_writer.write(line + \"\\n\")\n",
    "            f_writer.flush()\n",
    "f_writer.close()\n",
    "print(len(raw_word_list))\n",
    "print(len(set(raw_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 文本编码，通过汉字找到相应的编码，通过编码找到相应的汉字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据编码找到相应的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 41043\n",
      "江南\n",
      "[1983, 13891, 20228, 20229, 20230, 7412, 4195, 13892, 20231, 20232, 20233, 13893, 13894, 478, 1810, 10701, 5699, 20234, 5700, 20235, 427, 2433, 10701, 906, 20236, 7413, 20237, 20238, 20239, 3332, 2781, 8739, 251, 2954, 20240, 8740, 7414, 2093, 7415, 10702, 4615, 2434, 1984, 13895, 2094, 3861, 10703, 7416, 13896, 13897, 598, 10703, 8741, 20241, 10703, 509, 1260, 1030, 4615, 2434, 20242, 20243, 132, 20244, 2434, 367, 203, 510, 20245, 13898, 13899, 13900, 4196, 13901, 7417, 839, 5701, 2186, 2435, 20246, 7418, 20247, 10704, 20248, 367, 203, 510, 20249, 2187, 20250, 20251, 105, 6485, 20252, 20253, 5702, 20254, 1622, 10705, 8742, 100, 689, 13902, 13903, 6486, 485, 20255, 2619, 74, 7419, 1559, 587, 1622, 100, 330, 48, 10706, 839, 36, 203, 10707, 4616, 3333, 1811, 20256, 839, 200, 2782, 690, 213, 2783, 20257, 20258, 3334, 4617, 20259, 5091, 551, 3135, 8743, 324, 132, 3335, 10708, 1984, 13904, 1119, 2784, 2785, 4618, 13905, 3565, 2436, 2436, 2188, 2955, 434, 1887, 10709, 20260, 10710, 3566, 20261, 1003, 20262, 203, 1812, 1622, 100, 1227, 371, 195, 1985, 5703, 551, 152, 4197, 2319, 320, 2786, 20263, 7420, 5704, 10711, 8744, 5705, 1394, 288, 100, 1560, 3567, 1813, 5092, 453, 5093, 10712, 551, 434, 288, 2620]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(set(raw_word_list))\n",
    "words = raw_word_list\n",
    "count =[['UNK', '-1']]\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "print(\"count\",len(count))\n",
    "dictionary = dict()\n",
    "\n",
    "for word, _ in count:\n",
    "    dictionary[word]=len(dictionary)\n",
    "data=list()\n",
    "unk_count = 0\n",
    "for word in words:\n",
    "    if word in dictionary:\n",
    "        index = dictionary[word]\n",
    "    else:\n",
    "        index = 0\n",
    "        unk_count = unk_count+1\n",
    "    data.append(index)\n",
    "count[0][1] = unk_count\n",
    "\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "del words\n",
    "print(reverse_dictionary[1000])\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型搭建与测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这部分不仅仅包含Skip-gram模型的搭建，还包括参数设置：skip_window设置为2，其为单词最远可以联系的距离，每个单词生成样本数设置为4，也就是num_skips为4，batch_size设置为128...,然后进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20228 金庸 --> 13891 作者\n",
      "20228 金庸 --> 20229 灭门\n",
      "20228 金庸 --> 1983 笑傲江湖\n",
      "20228 金庸 --> 20230 风熏\n",
      "20229 灭门 --> 20228 金庸\n",
      "20229 灭门 --> 7412 柳\n",
      "20229 灭门 --> 20230 风熏\n",
      "20229 灭门 --> 13891 作者\n",
      "20230 风熏 --> 20228 金庸\n",
      "20230 风熏 --> 4195 花香\n",
      "step: 0, loss: 605.019836\n",
      "与令狐冲最近的前10词是 禅院, 余一片, 报答, 一手交货, 真有假, 五十两, 中加, 退后, 一了百了, 有定,\n",
      "与左冷禅最近的前10词是 很长, 收殓, 大有分别, 绞索, 老起, 陷愈, 驻足, 小兵, 此剑, 左肘,\n",
      "与林平之最近的前10词是 滋补, 黑狱, 单请, 本寺, 辰, 性命不保, 造反派, 起伏不定, 易发见, 食物,\n",
      "与岳不群最近的前10词是 苏, 稍加, 石几, 邻家, 冲进去, 立地成佛, 有道是, 倚傍, 同在, 尝尝,\n",
      "与桃根仙最近的前10词是 以小人之心, 上下左右, 俘虏, 刮去, 长城, 越众, 从练, 乐队, 把手, 括,\n",
      "step: 5000, loss: 489.685852\n",
      "step: 10000, loss: 423.761383\n",
      "与令狐冲最近的前10词是 岳不群, 弟子, 田伯光, 师父, 决在, 中加, 盈盈, 允, 林平之, 受伤,\n",
      "与左冷禅最近的前10词是 很长, 绞索, 收殓, 陷愈, 此剑, 大有分别, 小兵, 学到, 好人, 疯子,\n",
      "与林平之最近的前10词是 滋补, 造反派, 令狐冲, 黑狱, 土语, 六个, 结亲, 来处, 气助, 摇摇欲坠,\n",
      "与岳不群最近的前10词是 令狐冲, 他点, 苏, 石几, 以强凌弱, 激得, 稍加, 招斗, 听得见, 中任击,\n",
      "与桃根仙最近的前10词是 以小人之心, 上下左右, 俘虏, 长城, 从练, 把手, 一蒸, 刮去, 越众, 道袍,\n",
      "step: 15000, loss: 374.860016\n",
      "step: 20000, loss: 403.257141\n",
      "与令狐冲最近的前10词是 弟子, 岳不群, 师父, 剑法, 岳灵珊, 盈盈, 林平之, 瞧, 恒山, 田伯光,\n",
      "与左冷禅最近的前10词是 很长, 此剑, 绞索, 陷愈, 收殓, 令狐冲, 好人, 伸手, 醺醺然, 大有分别,\n",
      "与林平之最近的前10词是 令狐冲, 师父, 岳不群, 盈盈, 弟子, 岳灵珊, 造反派, 六个, 滋补, 师妹,\n",
      "与岳不群最近的前10词是 令狐冲, 盈盈, 岳灵珊, 心想, 仪琳, 剑法, 瞧, 林平之, 长剑, 见到,\n",
      "与桃根仙最近的前10词是 俘虏, 以小人之心, 上下左右, 从练, 长城, 一蒸, 把手, 道袍, 又理, 括,\n",
      "step: 25000, loss: 270.457489\n",
      "step: 30000, loss: 334.100098\n",
      "与令狐冲最近的前10词是 岳不群, 弟子, 剑法, 师父, 盈盈, 林平之, 岳灵珊, 恒山, 瞧, 田伯光,\n",
      "与左冷禅最近的前10词是 令狐冲, 弟子, 恒山, 师父, 五岳, 岳不群, 剑法, 很长, 盈盈, 伸手,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 师父, 弟子, 剑法, 盈盈, 岳灵珊, 长剑, 恒山, 心中,\n",
      "与岳不群最近的前10词是 令狐冲, 盈盈, 剑法, 弟子, 岳灵珊, 师父, 林平之, 仪琳, 恒山, 瞧,\n",
      "与桃根仙最近的前10词是 俘虏, 上下左右, 以小人之心, 从练, 胸口, 长城, 又理, 把手, 一蒸, 道袍,\n",
      "step: 35000, loss: 410.839691\n",
      "step: 40000, loss: 339.517181\n",
      "与令狐冲最近的前10词是 岳不群, 弟子, 师父, 剑法, 岳灵珊, 林平之, 盈盈, 恒山, 田伯光, 瞧,\n",
      "与左冷禅最近的前10词是 令狐冲, 师父, 弟子, 岳不群, 恒山, 剑法, 五岳, 岳灵珊, 盈盈, 林平之,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 师父, 弟子, 剑法, 岳灵珊, 盈盈, 田伯光, 恒山, 长剑,\n",
      "与岳不群最近的前10词是 令狐冲, 剑法, 师父, 弟子, 盈盈, 岳灵珊, 林平之, 田伯光, 恒山, 仪琳,\n",
      "与桃根仙最近的前10词是 师父, 胸口, 倒, 岳不群, 任我行, 上下左右, 从练, 俘虏, 未必, 令狐冲,\n",
      "step: 45000, loss: 232.210312\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    #声明全局变量\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "\n",
    "    #对某个单词创建相关样本时使用到的单词数量\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch,labels\n",
    "batch, labels = generate_batch(batch_size=128 , num_skips=4 , skip_window=2)\n",
    "\n",
    "for i in range(10):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], \"-->\", labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "#skip-gram model\n",
    "batch_size = 128  \n",
    "embedding_size = 300\n",
    "skip_window = 2  #\n",
    "num_skips = 4  \n",
    "valid_window = 100\n",
    "num_sample = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "#校验集\n",
    "valid_word = ['令狐冲', '左冷禅', '林平之', '岳不群', '桃根仙']\n",
    "valid_example = [dictionary[li] for li in valid_word]\n",
    "\n",
    "#定义skip-gram网络结构\n",
    "data_index = 0\n",
    "\n",
    "# 为skip-gram模型生成训练批次\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    # 得到窗口长度( 当前单词左边和右边 + 当前单词)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "\n",
    "    #回溯一点，以避免在批处理结束时跳过单词\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "# 确保在CPU上分配以下操作和变量\n",
    "# (某些操作在GPU上不兼容)\n",
    "with tf.device('/cpu:0'):\n",
    "    # 创建嵌入变量（每一行代表一个词嵌入向量） embedding vector).\n",
    "    embedding = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
    "    # 构造NCE损失的变量\n",
    "    nce_weights = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "def get_embedding(x):\n",
    "    with tf.device('/cpu:0'):\n",
    "       # 对于X中的每一个样本查找对应的嵌入向量\n",
    "        x_embed = tf.nn.embedding_lookup(embedding, x)\n",
    "        return x_embed\n",
    "\n",
    "def nce_loss(x_embed, y):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # 计算批处理的平均NCE损失\n",
    "        y = tf.cast(y, tf.int64)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights=nce_weights,\n",
    "                           biases=nce_biases,\n",
    "                           labels=y,\n",
    "                           inputs=x_embed,\n",
    "                           num_sampled=num_sample,\n",
    "                           num_classes=vocabulary_size))\n",
    "        return loss\n",
    "\n",
    "# 评估\n",
    "def evaluate(x_embed):\n",
    "    with tf.device('/cpu:0'):\n",
    "         # 计算输入数据嵌入与每个嵌入向量之间的余弦相似度\n",
    "        x_embed = tf.cast(x_embed, tf.float32)\n",
    "        x_embed_norm = x_embed / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
    "        embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True), tf.float32)\n",
    "        cosine_sim_op = tf.matmul(x_embed_norm, embedding_norm, transpose_b=True)\n",
    "        return cosine_sim_op\n",
    "\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# 优化过程\n",
    "def run_optimization(x, y):\n",
    "    with tf.device('/cpu:0'):\n",
    "       # 将计算封装在GradientTape中以实现自动微分\n",
    "        with tf.GradientTape() as g:\n",
    "            emb = get_embedding(x)\n",
    "            loss = nce_loss(emb, y)\n",
    "\n",
    "        # 计算梯度\n",
    "        gradients = g.gradient(loss, [embedding, nce_weights, nce_biases])\n",
    "\n",
    "         # 按gradients更新 W 和 b\n",
    "        optimizer.apply_gradients(zip(gradients, [embedding, nce_weights, nce_biases]))\n",
    "\n",
    "\n",
    "# 用于测试的单词\n",
    "x_test = np.array(valid_example)\n",
    "num_steps = 2000000\n",
    "avg_loss = 0\n",
    "# 针对给定步骤数进行训练\n",
    "for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "    run_optimization(batch_inputs, batch_labels)\n",
    "    loss = nce_loss(get_embedding(batch_inputs), batch_labels)\n",
    "    avg_loss = avg_loss + loss\n",
    "\n",
    "    if step % 5000 == 0:\n",
    "        if step > 0:\n",
    "            avg_loss = avg_loss / 5000\n",
    "        loss = nce_loss(get_embedding(batch_inputs), batch_labels)\n",
    "        print(\"step: %i, loss: %f\" % (step, loss))\n",
    "        # print(\"平均损失在\", num_steps, \"中为：\", avg_loss)\n",
    "\n",
    "    # 计算验证集合的相似度\n",
    "    if step % 10000 == 0:\n",
    "        sim = evaluate(get_embedding(x_test)).numpy()\n",
    "        for i in range(len(valid_word)):\n",
    "            val_word = reverse_dictionary[valid_example[i]]\n",
    "            top_k = 10\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "            sim_str = \"与\" + val_word + \"最近的前10词是\"\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                sim_str = \"%s %s,\" % (sim_str, close_word)\n",
    "            print(sim_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
