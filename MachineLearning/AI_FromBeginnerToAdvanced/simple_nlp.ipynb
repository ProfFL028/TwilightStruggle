{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1909\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "stop_words = []\n",
    "with open(\"./stop_words.txt\", \"r\", encoding='utf-8') as f_stopwords:\n",
    "    for line in f_stopwords:\n",
    "        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "        stop_words.append(line)\n",
    "stop_words = set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\proffl\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.102 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary count: 41043\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "\n",
    "chinese_regular = u\"([\\u4e00-\\u9fa5]+)\"\n",
    "pattern = re.compile(chinese_regular)\n",
    "\n",
    "f_writer = open(\"./data/seg_The_Smiling_Proud_Wanderer.txt\", \"w\", encoding=\"utf-8\")\n",
    "raw_word_list = []\n",
    "with open(\"./data/The_Smiling_Proud_Wanderer.txt\", \"r\", encoding=\"utf-8\") as f_reader:\n",
    "    for line in f_reader:\n",
    "        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        line = \" \".join(jieba.cut(line))\n",
    "        chinese_list = pattern.findall(line)\n",
    "        word_list = []\n",
    "        for word in chinese_list:\n",
    "            if word not in stop_words:\n",
    "                word_list.append(word)\n",
    "        if len(word_list) > 0:\n",
    "            raw_word_list.extend(word_list)\n",
    "            line = \" \".join(word_list)\n",
    "            f_writer.write(line + \"\\n\")\n",
    "            f_writer.flush()\n",
    "f_writer.close()\n",
    "vocabulary_set = set(raw_word_list)\n",
    "print(\"vocabulary count:\", len(vocabulary_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-18-0339f77114a4>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcorpora\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdictionary\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcorpora\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDictionary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocabulary_set\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mdictionary\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdocuments\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 78\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_documents\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocuments\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprune_at\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprune_at\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     79\u001B[0m             self.add_lifecycle_event(\n\u001B[0;32m     80\u001B[0m                 \u001B[1;34m\"created\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001B[0m in \u001B[0;36madd_documents\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m    200\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    201\u001B[0m             \u001B[1;31m# update Dictionary with the document\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 202\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdoc2bow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocument\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mallow_update\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# ignore the result, here we only care about updating token ids\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    203\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    204\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"built %s from %i documents (total %i corpus positions)\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_docs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_pos\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001B[0m in \u001B[0;36mdoc2bow\u001B[1;34m(self, document, allow_update, return_missing)\u001B[0m\n\u001B[0;32m    237\u001B[0m         \"\"\"\n\u001B[0;32m    238\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocument\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 239\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    240\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    241\u001B[0m         \u001B[1;31m# Construct (word, frequency) mapping.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "vocabulary_size = len(vocabulary_set)\n",
    "words = raw_word_list\n",
    "count = [['UNK', '-1']]\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "print(\"count\", len(count))\n",
    "count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "江南\n",
      "[1983, 13891, 20228, 20229, 20230, 7412, 4195, 13892, 20231, 20232, 20233, 13893, 13894, 478, 1810, 10701, 5699, 20234, 5700, 20235, 427, 2433, 10701, 906, 20236, 7413, 20237, 20238, 20239, 3332, 2781, 8739, 251, 2954, 20240, 8740, 7414, 2093, 7415, 10702, 4615, 2434, 1984, 13895, 2094, 3861, 10703, 7416, 13896, 13897, 598, 10703, 8741, 20241, 10703, 509, 1260, 1030, 4615, 2434, 20242, 20243, 132, 20244, 2434, 367, 203, 510, 20245, 13898, 13899, 13900, 4196, 13901, 7417, 839, 5701, 2186, 2435, 20246, 7418, 20247, 10704, 20248, 367, 203, 510, 20249, 2187, 20250, 20251, 105, 6485, 20252, 20253, 5702, 20254, 1622, 10705, 8742, 100, 689, 13902, 13903, 6486, 485, 20255, 2619, 74, 7419, 1559, 587, 1622, 100, 330, 48, 10706, 839, 36, 203, 10707, 4616, 3333, 1811, 20256, 839, 200, 2782, 690, 213, 2783, 20257, 20258, 3334, 4617, 20259, 5091, 551, 3135, 8743, 324, 132, 3335, 10708, 1984, 13904, 1119, 2784, 2785, 4618, 13905, 3565, 2436, 2436, 2188, 2955, 434, 1887, 10709, 20260, 10710, 3566, 20261, 1003, 20262, 203, 1812, 1622, 100, 1227, 371, 195, 1985, 5703, 551, 152, 4197, 2319, 320, 2786, 20263, 7420, 5704, 10711, 8744, 5705, 1394, 288, 100, 1560, 3567, 1813, 5092, 453, 5093, 10712, 551, 434, 288, 2620]\n"
     ]
    }
   ],
   "source": [
    "dictionary = dict()\n",
    "for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "data = list()\n",
    "unk_count = 0\n",
    "for word in words:\n",
    "    if word in dictionary:\n",
    "        index = dictionary[word]\n",
    "    else:\n",
    "        index = 0\n",
    "        unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "count[0][1] = unk_count\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "del words\n",
    "print(reverse_dictionary[1000])\n",
    "print(data[:200])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    batch = np.ndarray(shape=batch_size, dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "batch, labels = generate_batch(batch_size=128, num_skips=4, skip_window=2)\n",
    "\n",
    "# skip-gram model\n",
    "batch_size = 128\n",
    "embedding_size = 300\n",
    "skip_window = 2\n",
    "num_skips = 4\n",
    "valid_window = 100\n",
    "num_sample = 64\n",
    "learning_rate = 0.01\n",
    "valid_word = ['令狐冲', '左冷禅', '林平之', '岳不群', '桃根仙']\n",
    "valid_example = [dictionary[li] for li in valid_word]\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    # 得到窗口长度( 当前单词左边和右边 + 当前单词)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "\n",
    "    #回溯一点，以避免在批处理结束时跳过单词\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 15000, loss: 449.917725\n",
      "step: 20000, loss: 382.382935\n",
      "与令狐冲最近的前10词是 岳不群, 剑法, 师父, 弟子, 岳灵珊, 仪琳, 恒山, 林平之, 盈盈, 田伯光,\n",
      "与左冷禅最近的前10词是 罢, 半碗, 余丈, 再也, 受, 令狐冲, 围杀, 师公, 叫惯, 出其不意,\n",
      "与林平之最近的前10词是 令狐冲, 甚烦, 一枝, 绝未, 岳不群, 岳灵珊, 用个, 坚硬, 走走, 向问天,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 剑法, 师父, 岳灵珊, 余沧海, 师哥, 盈盈, 长剑, 仪琳,\n",
      "与桃根仙最近的前10词是 方形, 着重点, 糊糊, 严惩不贷, 废去, 入门, 谢上, 之纯, 往上, 左掌猛,\n",
      "step: 25000, loss: 321.458496\n",
      "step: 30000, loss: 275.788727\n",
      "与令狐冲最近的前10词是 岳不群, 剑法, 师父, 弟子, 林平之, 盈盈, 岳灵珊, 恒山, 长剑, 仪琳,\n",
      "与左冷禅最近的前10词是 令狐冲, 岳不群, 恒山, 婆婆, 罢, 教主, 瞧, 师父, 死, 受,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 弟子, 剑法, 师父, 岳灵珊, 盈盈, 恒山, 武功, 师妹,\n",
      "与岳不群最近的前10词是 令狐冲, 剑法, 弟子, 师父, 盈盈, 岳灵珊, 林平之, 恒山, 长剑, 仪琳,\n",
      "与桃根仙最近的前10词是 方形, 着重点, 糊糊, 严惩不贷, 废去, 早已, 谢上, 入门, 之纯, 绝大,\n",
      "step: 35000, loss: 322.641663\n",
      "step: 40000, loss: 300.589600\n",
      "与令狐冲最近的前10词是 岳不群, 师父, 弟子, 剑法, 林平之, 岳灵珊, 盈盈, 恒山, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 令狐冲, 岳不群, 恒山, 师父, 弟子, 瞧, 剑法, 婆婆, 仪琳, 教主,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 师父, 弟子, 剑法, 岳灵珊, 盈盈, 恒山, 田伯光, 长剑,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 师父, 剑法, 盈盈, 林平之, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与桃根仙最近的前10词是 方形, 着重点, 糊糊, 早已, 令狐冲, 左冷禅, 岳不群, 严惩不贷, 废去, 只见,\n",
      "step: 45000, loss: 321.785278\n",
      "step: 50000, loss: 225.426544\n",
      "与令狐冲最近的前10词是 岳不群, 师父, 弟子, 剑法, 林平之, 盈盈, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 令狐冲, 岳不群, 恒山, 剑法, 师父, 弟子, 盈盈, 长剑, 瞧, 教主,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 师父, 弟子, 剑法, 盈盈, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 师父, 剑法, 盈盈, 林平之, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与桃根仙最近的前10词是 令狐冲, 左冷禅, 岳不群, 方形, 事, 早已, 剑法, 岳灵珊, 只见, 着重点,\n",
      "step: 55000, loss: 295.616150\n",
      "step: 60000, loss: 273.483368\n",
      "与令狐冲最近的前10词是 岳不群, 弟子, 师父, 剑法, 盈盈, 林平之, 岳灵珊, 恒山, 长剑, 仪琳,\n",
      "与左冷禅最近的前10词是 岳不群, 令狐冲, 弟子, 剑法, 恒山, 师父, 盈盈, 林平之, 长剑, 瞧,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 师父, 剑法, 弟子, 盈盈, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 剑法, 师父, 盈盈, 林平之, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与桃根仙最近的前10词是 岳不群, 令狐冲, 左冷禅, 事, 剑法, 岳灵珊, 恒山, 长剑, 向问天, 时,\n",
      "step: 65000, loss: 210.848816\n",
      "step: 70000, loss: 265.202148\n",
      "与令狐冲最近的前10词是 岳不群, 剑法, 师父, 弟子, 盈盈, 林平之, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 岳不群, 令狐冲, 剑法, 弟子, 恒山, 盈盈, 师父, 长剑, 林平之, 岳灵珊,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 师父, 剑法, 弟子, 岳灵珊, 盈盈, 恒山, 长剑, 田伯光,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 剑法, 师父, 盈盈, 林平之, 岳灵珊, 长剑, 恒山, 田伯光,\n",
      "与桃根仙最近的前10词是 岳不群, 令狐冲, 岳灵珊, 事, 长剑, 左冷禅, 恒山, 弟子, 向问天, 剑法,\n",
      "step: 75000, loss: 233.607330\n",
      "step: 80000, loss: 281.125061\n",
      "与令狐冲最近的前10词是 岳不群, 师父, 弟子, 剑法, 盈盈, 林平之, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 岳不群, 令狐冲, 恒山, 弟子, 盈盈, 剑法, 长剑, 师父, 林平之, 岳灵珊,\n",
      "与林平之最近的前10词是 令狐冲, 岳不群, 岳灵珊, 弟子, 师父, 剑法, 盈盈, 恒山, 长剑, 田伯光,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 剑法, 师父, 盈盈, 林平之, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与桃根仙最近的前10词是 岳不群, 令狐冲, 左冷禅, 岳灵珊, 长剑, 弟子, 恒山, 事, 盈盈, 向问天,\n",
      "step: 85000, loss: 202.689941\n",
      "step: 90000, loss: 206.358749\n",
      "与令狐冲最近的前10词是 岳不群, 盈盈, 师父, 弟子, 岳灵珊, 林平之, 剑法, 恒山, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 恒山, 岳不群, 令狐冲, 师父, 盈盈, 长剑, 剑法, 林平之, 弟子, 瞧,\n",
      "与林平之最近的前10词是 令狐冲, 岳灵珊, 岳不群, 弟子, 盈盈, 师父, 剑法, 长剑, 恒山, 田伯光,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 师父, 盈盈, 剑法, 林平之, 岳灵珊, 恒山, 长剑, 田伯光,\n",
      "与桃根仙最近的前10词是 左冷禅, 恒山, 岳不群, 弟子, 长剑, 事, 盈盈, 令狐冲, 向问天, 瞧,\n",
      "step: 95000, loss: 185.696625\n",
      "step: 100000, loss: 168.842300\n",
      "与令狐冲最近的前10词是 岳不群, 盈盈, 师父, 弟子, 岳灵珊, 林平之, 剑法, 长剑, 恒山, 田伯光,\n",
      "与左冷禅最近的前10词是 恒山, 师父, 岳不群, 长剑, 盈盈, 弟子, 剑法, 林平之, 令狐冲, 瞧,\n",
      "与林平之最近的前10词是 岳不群, 岳灵珊, 令狐冲, 弟子, 盈盈, 师父, 剑法, 长剑, 恒山, 田伯光,\n",
      "与岳不群最近的前10词是 弟子, 令狐冲, 师父, 盈盈, 林平之, 剑法, 岳灵珊, 长剑, 恒山, 田伯光,\n",
      "与桃根仙最近的前10词是 左冷禅, 弟子, 恒山, 向问天, 事, 长剑, 岳不群, 瞧, 盈盈, 令狐冲,\n",
      "step: 105000, loss: 191.355209\n",
      "step: 110000, loss: 249.652634\n",
      "与令狐冲最近的前10词是 岳不群, 盈盈, 弟子, 岳灵珊, 师父, 剑法, 林平之, 恒山, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 盈盈, 恒山, 岳不群, 师父, 令狐冲, 弟子, 林平之, 瞧, 长剑, 剑法,\n",
      "与林平之最近的前10词是 岳灵珊, 岳不群, 盈盈, 弟子, 剑法, 令狐冲, 师父, 恒山, 长剑, 田伯光,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 弟子, 盈盈, 林平之, 剑法, 岳灵珊, 恒山, 长剑, 仪琳,\n",
      "与桃根仙最近的前10词是 左冷禅, 瞧, 弟子, 恒山, 盈盈, 长剑, 事, 令狐冲, 向问天, 林平之,\n",
      "step: 115000, loss: 239.788589\n",
      "step: 120000, loss: 242.129913\n",
      "与令狐冲最近的前10词是 盈盈, 岳不群, 弟子, 岳灵珊, 师父, 林平之, 剑法, 恒山, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 盈盈, 恒山, 岳不群, 师父, 林平之, 弟子, 长剑, 瞧, 岳灵珊, 令狐冲,\n",
      "与林平之最近的前10词是 盈盈, 岳灵珊, 岳不群, 令狐冲, 弟子, 师父, 恒山, 剑法, 长剑, 田伯光,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 弟子, 林平之, 盈盈, 岳灵珊, 剑法, 恒山, 长剑, 仪琳,\n",
      "与桃根仙最近的前10词是 左冷禅, 弟子, 瞧, 武功, 事, 长剑, 向问天, 林平之, 恒山, 心中,\n",
      "step: 125000, loss: 180.898376\n",
      "step: 130000, loss: 212.248535\n",
      "与令狐冲最近的前10词是 岳不群, 师父, 盈盈, 弟子, 岳灵珊, 林平之, 恒山, 剑法, 长剑, 田伯光,\n",
      "与左冷禅最近的前10词是 恒山, 岳不群, 盈盈, 师父, 剑法, 瞧, 林平之, 令狐冲, 弟子, 仪琳,\n",
      "与林平之最近的前10词是 岳灵珊, 盈盈, 师父, 岳不群, 令狐冲, 恒山, 弟子, 剑法, 长剑, 瞧,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 盈盈, 弟子, 剑法, 岳灵珊, 林平之, 恒山, 长剑, 仪琳,\n",
      "与桃根仙最近的前10词是 左冷禅, 瞧, 弟子, 令狐冲, 向问天, 岳灵珊, 长剑, 武功, 事, 林平之,\n",
      "step: 135000, loss: 148.795837\n",
      "step: 140000, loss: 195.776672\n",
      "与令狐冲最近的前10词是 盈盈, 岳不群, 师父, 弟子, 岳灵珊, 恒山, 林平之, 长剑, 剑法, 田伯光,\n",
      "与左冷禅最近的前10词是 恒山, 剑法, 令狐冲, 盈盈, 师父, 岳不群, 瞧, 岳灵珊, 弟子, 林平之,\n",
      "与林平之最近的前10词是 岳灵珊, 盈盈, 师父, 岳不群, 令狐冲, 剑法, 弟子, 恒山, 长剑, 瞧,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 盈盈, 岳灵珊, 弟子, 恒山, 林平之, 长剑, 剑法, 瞧,\n",
      "与桃根仙最近的前10词是 左冷禅, 令狐冲, 瞧, 弟子, 事, 岳灵珊, 心想, 向问天, 盈盈, 时,\n",
      "step: 145000, loss: 138.113297\n",
      "step: 150000, loss: 163.614807\n",
      "与令狐冲最近的前10词是 盈盈, 师父, 岳不群, 岳灵珊, 林平之, 弟子, 剑法, 恒山, 长剑, 瞧,\n",
      "与左冷禅最近的前10词是 恒山, 盈盈, 令狐冲, 师父, 瞧, 弟子, 剑法, 岳灵珊, 仪琳, 五岳,\n",
      "与林平之最近的前10词是 岳灵珊, 盈盈, 令狐冲, 师父, 剑法, 岳不群, 弟子, 恒山, 长剑, 瞧,\n",
      "与岳不群最近的前10词是 盈盈, 令狐冲, 师父, 弟子, 岳灵珊, 林平之, 恒山, 长剑, 剑法, 瞧,\n",
      "与桃根仙最近的前10词是 左冷禅, 弟子, 事, 令狐冲, 瞧, 岳灵珊, 心想, 盈盈, 时, 向问天,\n",
      "step: 155000, loss: 139.817230\n",
      "step: 160000, loss: 137.693298\n",
      "与令狐冲最近的前10词是 盈盈, 岳不群, 岳灵珊, 师父, 林平之, 弟子, 仪琳, 瞧, 恒山, 田伯光,\n",
      "与左冷禅最近的前10词是 盈盈, 恒山, 瞧, 仪琳, 岳灵珊, 令狐冲, 教主, 弟子, 心中, 林平之,\n",
      "与林平之最近的前10词是 岳灵珊, 师父, 盈盈, 令狐冲, 剑法, 弟子, 岳不群, 瞧, 长剑, 恒山,\n",
      "与岳不群最近的前10词是 盈盈, 令狐冲, 弟子, 师父, 岳灵珊, 林平之, 剑法, 恒山, 长剑, 瞧,\n",
      "与桃根仙最近的前10词是 左冷禅, 事, 盈盈, 向问天, 时, 弟子, 瞧, 心想, 令狐冲, 林平之,\n",
      "step: 165000, loss: 158.146484\n",
      "step: 170000, loss: 160.286316\n",
      "与令狐冲最近的前10词是 盈盈, 弟子, 师父, 岳不群, 岳灵珊, 恒山, 林平之, 瞧, 仪琳, 剑法,\n",
      "与左冷禅最近的前10词是 恒山, 盈盈, 弟子, 令狐冲, 岳灵珊, 仪琳, 五岳, 师父, 瞧, 教主,\n",
      "与林平之最近的前10词是 岳灵珊, 令狐冲, 师父, 剑法, 岳不群, 弟子, 盈盈, 瞧, 恒山, 长剑,\n",
      "与岳不群最近的前10词是 令狐冲, 弟子, 盈盈, 师父, 岳灵珊, 林平之, 剑法, 长剑, 恒山, 仪琳,\n",
      "与桃根仙最近的前10词是 左冷禅, 事, 令狐冲, 向问天, 时, 盈盈, 林平之, 弟子, 教主, 瞧,\n",
      "step: 175000, loss: 136.894882\n",
      "step: 180000, loss: 152.414490\n",
      "与令狐冲最近的前10词是 岳不群, 弟子, 盈盈, 岳灵珊, 林平之, 师父, 恒山, 剑法, 仪琳, 瞧,\n",
      "与左冷禅最近的前10词是 恒山, 岳灵珊, 盈盈, 令狐冲, 瞧, 嵩山, 五岳, 林平之, 弟子, 心中,\n",
      "与林平之最近的前10词是 岳灵珊, 师父, 岳不群, 剑法, 令狐冲, 盈盈, 弟子, 瞧, 恒山, 长剑,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 岳灵珊, 弟子, 林平之, 盈盈, 剑法, 田伯光, 长剑, 恒山,\n",
      "与桃根仙最近的前10词是 左冷禅, 事, 向问天, 林平之, 教主, 时, 盈盈, 心想, 令狐冲, 弟子,\n",
      "step: 185000, loss: 118.185707\n",
      "step: 190000, loss: 90.886780\n",
      "与令狐冲最近的前10词是 盈盈, 岳不群, 岳灵珊, 林平之, 师父, 仪琳, 长剑, 恒山, 弟子, 剑法,\n",
      "与左冷禅最近的前10词是 盈盈, 恒山, 瞧, 五岳, 岳灵珊, 心想, 令狐冲, 嵩山, 心中, 师父,\n",
      "与林平之最近的前10词是 岳灵珊, 令狐冲, 师父, 岳不群, 剑法, 盈盈, 瞧, 师妹, 恒山, 弟子,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 岳灵珊, 林平之, 盈盈, 弟子, 剑法, 长剑, 田伯光, 心中,\n",
      "与桃根仙最近的前10词是 教主, 左冷禅, 向问天, 任我行, 事, 心想, 时, 武功, 弟子, 死,\n",
      "step: 195000, loss: 162.757446\n",
      "step: 200000, loss: 127.147247\n",
      "与令狐冲最近的前10词是 盈盈, 林平之, 岳不群, 师父, 岳灵珊, 仪琳, 恒山, 瞧, 弟子, 心中,\n",
      "与左冷禅最近的前10词是 盈盈, 瞧, 五岳, 恒山, 令狐冲, 心想, 嵩山, 林平之, 武功, 心中,\n",
      "与林平之最近的前10词是 令狐冲, 岳灵珊, 师父, 岳不群, 盈盈, 剑法, 瞧, 心想, 恒山, 师妹,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 林平之, 弟子, 岳灵珊, 盈盈, 田伯光, 长剑, 恒山, 仪琳,\n",
      "与桃根仙最近的前10词是 教主, 左冷禅, 向问天, 任我行, 事, 心想, 倒, 恒山, 时, 弟子,\n",
      "step: 205000, loss: 145.438751\n",
      "step: 210000, loss: 124.754082\n",
      "与令狐冲最近的前10词是 岳不群, 盈盈, 岳灵珊, 林平之, 师父, 瞧, 恒山, 弟子, 仪琳, 心想,\n",
      "与左冷禅最近的前10词是 恒山, 盈盈, 瞧, 五岳, 令狐冲, 心想, 剑法, 嵩山, 岳灵珊, 师父,\n",
      "与林平之最近的前10词是 岳灵珊, 师父, 岳不群, 令狐冲, 盈盈, 剑法, 心想, 师妹, 仪琳, 瞧,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 林平之, 盈盈, 岳灵珊, 弟子, 仪琳, 田伯光, 心想, 心中,\n",
      "与桃根仙最近的前10词是 向问天, 教主, 任我行, 左冷禅, 恒山, 时, 心想, 事, 死, 倒,\n",
      "step: 215000, loss: 160.656418\n",
      "step: 220000, loss: 109.202759\n",
      "与令狐冲最近的前10词是 盈盈, 岳不群, 岳灵珊, 林平之, 师父, 弟子, 瞧, 心想, 恒山, 师妹,\n",
      "与左冷禅最近的前10词是 盈盈, 心想, 恒山, 令狐冲, 五岳, 剑法, 嵩山, 瞧, 武功, 师父,\n",
      "与林平之最近的前10词是 岳灵珊, 令狐冲, 岳不群, 师父, 盈盈, 心想, 剑法, 仪琳, 师妹, 瞧,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 林平之, 岳灵珊, 弟子, 盈盈, 心想, 心中, 瞧, 仪琳,\n",
      "与桃根仙最近的前10词是 向问天, 任我行, 左冷禅, 教主, 倒, 恒山, 心想, 时, 事, 五岳,\n",
      "step: 225000, loss: 119.692963\n",
      "step: 230000, loss: 127.599091\n",
      "与令狐冲最近的前10词是 岳不群, 盈盈, 岳灵珊, 林平之, 师父, 弟子, 心想, 瞧, 心中, 仪琳,\n",
      "与左冷禅最近的前10词是 盈盈, 心想, 令狐冲, 五岳, 恒山, 嵩山, 瞧, 岳不群, 林平之, 师父,\n",
      "与林平之最近的前10词是 岳灵珊, 令狐冲, 师父, 岳不群, 盈盈, 剑法, 心想, 师妹, 仪琳, 瞧,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 林平之, 岳灵珊, 盈盈, 弟子, 心中, 瞧, 心想, 仪琳,\n",
      "与桃根仙最近的前10词是 向问天, 令狐冲, 教主, 左冷禅, 倒, 任我行, 恒山, 五岳, 心想, 事,\n",
      "step: 235000, loss: 82.336304\n",
      "step: 240000, loss: 125.824310\n",
      "与令狐冲最近的前10词是 岳灵珊, 岳不群, 盈盈, 师父, 林平之, 心想, 心中, 仪琳, 弟子, 恒山,\n",
      "与左冷禅最近的前10词是 心想, 令狐冲, 盈盈, 瞧, 五岳, 恒山, 嵩山, 师父, 便是, 岳不群,\n",
      "与林平之最近的前10词是 岳灵珊, 师父, 令狐冲, 岳不群, 盈盈, 心想, 剑法, 师妹, 仪琳, 恒山,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 林平之, 岳灵珊, 盈盈, 心中, 心想, 弟子, 瞧, 便是,\n",
      "与桃根仙最近的前10词是 向问天, 倒, 死, 教主, 任我行, 五岳, 恒山, 左冷禅, 令狐冲, 岳夫人,\n",
      "step: 245000, loss: 96.222343\n",
      "step: 250000, loss: 125.815048\n",
      "与令狐冲最近的前10词是 盈盈, 岳灵珊, 师父, 岳不群, 林平之, 心想, 心中, 弟子, 仪琳, 便是,\n",
      "与左冷禅最近的前10词是 盈盈, 心想, 令狐冲, 便是, 恒山, 师父, 瞧, 五岳, 岳灵珊, 岳不群,\n",
      "与林平之最近的前10词是 岳灵珊, 令狐冲, 师父, 岳不群, 盈盈, 心想, 师妹, 瞧, 便是, 仪琳,\n",
      "与岳不群最近的前10词是 令狐冲, 师父, 盈盈, 岳灵珊, 心中, 林平之, 心想, 弟子, 瞧, 便是,\n",
      "与桃根仙最近的前10词是 向问天, 倒, 任我行, 教主, 恒山, 左冷禅, 岳夫人, 只见, 五岳, 事,\n",
      "step: 255000, loss: 98.354172\n",
      "step: 260000, loss: 116.896843\n",
      "与令狐冲最近的前10词是 岳灵珊, 师父, 岳不群, 林平之, 盈盈, 心想, 便是, 师妹, 心中, 弟子,\n",
      "与左冷禅最近的前10词是 心想, 便是, 令狐冲, 恒山, 盈盈, 岳不群, 五岳, 岳灵珊, 武功, 瞧,\n",
      "与林平之最近的前10词是 岳灵珊, 令狐冲, 岳不群, 心想, 盈盈, 师父, 师妹, 瞧, 便是, 仪琳,\n",
      "与岳不群最近的前10词是 令狐冲, 盈盈, 师父, 岳灵珊, 林平之, 心想, 心中, 便是, 瞧, 弟子,\n",
      "与桃根仙最近的前10词是 向问天, 倒, 教主, 任我行, 事, 左冷禅, 只见, 恒山, 岳夫人, 田伯光,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # 创建嵌入变量（每一行代表一个词嵌入向量） embedding vector).\n",
    "    embedding = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
    "    # 构造NCE损失的变量\n",
    "    nce_weights = tf.Variable(tf.random.normal([vocabulary_size, embedding_size]))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "def get_embedding(x):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # 对于X中的每一个样本查找对应的嵌入向量\n",
    "        x_embed = tf.nn.embedding_lookup(embedding, x)\n",
    "        return x_embed\n",
    "\n",
    "\n",
    "def nce_loss(x_embed, y):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # 计算批处理的平均NCE损失\n",
    "        y = tf.cast(y, tf.int64)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights=nce_weights,\n",
    "                           biases=nce_biases,\n",
    "                           labels=y,\n",
    "                           inputs=x_embed,\n",
    "                           num_sampled=num_sample,\n",
    "                           num_classes=vocabulary_size))\n",
    "        return loss\n",
    "\n",
    "\n",
    "# 评估\n",
    "def evaluate(x_embed):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # 计算输入数据嵌入与每个嵌入向量之间的余弦相似度\n",
    "        x_embed = tf.cast(x_embed, tf.float32)\n",
    "        x_embed_norm = x_embed / tf.sqrt(tf.reduce_sum(tf.square(x_embed)))\n",
    "        embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True), tf.float32)\n",
    "        cosine_sim_op = tf.matmul(x_embed_norm, embedding_norm, transpose_b=True)\n",
    "        return cosine_sim_op\n",
    "\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "\n",
    "# 优化过程\n",
    "def run_optimization(x, y):\n",
    "    with tf.device('/cpu:0'):\n",
    "        # 将计算封装在GradientTape中以实现自动微分\n",
    "        with tf.GradientTape() as g:\n",
    "            emb = get_embedding(x)\n",
    "            loss = nce_loss(emb, y)\n",
    "\n",
    "        # 计算梯度\n",
    "        gradients = g.gradient(loss, [embedding, nce_weights, nce_biases])\n",
    "\n",
    "        # 按gradients更新 W 和 b\n",
    "        optimizer.apply_gradients(zip(gradients, [embedding, nce_weights, nce_biases]))\n",
    "\n",
    "\n",
    "# 用于测试的单词\n",
    "x_test = np.array(valid_example)\n",
    "num_steps = 2000000\n",
    "avg_loss = 0\n",
    "# 针对给定步骤数进行训练\n",
    "for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "    run_optimization(batch_inputs, batch_labels)\n",
    "    loss = nce_loss(get_embedding(batch_inputs), batch_labels)\n",
    "    avg_loss = avg_loss + loss\n",
    "\n",
    "    if step % 5000 == 0:\n",
    "        if step > 0:\n",
    "            avg_loss = avg_loss / 5000\n",
    "        loss = nce_loss(get_embedding(batch_inputs), batch_labels)\n",
    "        print(\"step: %i, loss: %f\" % (step, loss))\n",
    "        # print(\"平均损失在\", num_steps, \"中为：\", avg_loss)\n",
    "\n",
    "    # 计算验证集合的相似度\n",
    "    if step % 10000 == 0:\n",
    "        sim = evaluate(get_embedding(x_test)).numpy()\n",
    "        for i in range(len(valid_word)):\n",
    "            val_word = reverse_dictionary[valid_example[i]]\n",
    "            top_k = 10\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            sim_str = \"与\" + val_word + \"最近的前10词是\"\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                sim_str = \"%s %s,\" % (sim_str, close_word)\n",
    "            print(sim_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}