{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (4.6.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (4.48.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: dataclasses in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: filelock in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (0.0.8)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: requests in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (1.7.0)\n",
      "Requirement already satisfied: sacremoses in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in /home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Source:\n",
    "            https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(output):\n",
    "    logits = output[..., -1, :].squeeze(0)\n",
    "    logits = top_k_top_p_filtering(logits, top_k=10)\n",
    "    log_probs = torch.softmax(logits, dim=-1)\n",
    "    token = torch.multinomial(log_probs, num_samples=1)[0]\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:810: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "model = AutoModelWithLMHead.from_pretrained('transfo-xl-wt103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer.encode(\"On our way to the beach\")\n",
    "context = torch.tensor([generated])\n",
    "past = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    output = model(context, mems=past)\n",
    "    token = sample_token(output.prediction_scores)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = token.view(1, -1)\n",
    "    past = output.mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On our way to the beach on the morning of the weekend of his, <eos> the morning of the Friday the 13th is a very quiet day, and the afternoon of Saturday of the 13th is quite good. \"<eos> The day after Monday morning, Saturday afternoon, the afternoon of the 14th, a very quiet day of the 13th, is celebrated with a fireworks display and fireworks. <eos> The day after Saturday, Thursday afternoon, the night of the 14th, the afternoon of Friday is devoted to the celebration. The day is also dedicated to\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = AutoModelWithLMHead.from_pretrained('gpt2-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer.encode(\"On our way to the beach\")\n",
    "context = torch.tensor([generated])\n",
    "past = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    output = model(context, past_key_values=past)\n",
    "    token = sample_token(output.logits)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = token.unsqueeze(0)\n",
    "    past = output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On our way to the beach we found a few more people who were looking at the same thing that we were.\n",
      "\n",
      "I think we got about a thousand people in one night, and the first night was about a million.\n",
      "\n",
      "When we arrived back at the house, a few of those same people had joined our group.\n",
      "\n",
      "I was pretty sure we'd had the first big crowd.\n",
      "\n",
      "I was pretty sure that the first person I'd ever been close to was someone I'd met in the past.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-clm-enfr-1024 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-clm-enfr-1024')\n",
    "model = AutoModelWithLMHead.from_pretrained('xlm-clm-enfr-1024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = [0] # start with just <s>\n",
    "context = torch.tensor([generated])\n",
    "lang = 0 # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    langs = torch.zeros_like(context).fill_(lang)\n",
    "    output = model(context, langs=langs)\n",
    "    token = sample_token(output.logits)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = torch.tensor([generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>, and is a key driver of our financial results. \" </s>\" But we have made the decision to take it very carefully and that will be based on the best available evidence, \" Mr Hunt said. \" </s>\" We are looking at it. </s>It's been a difficult decision. </s>It's not about a lack of resources, \" he said. </s>The fact is that we're going to take the right decision. </s>He's the right time in his situation. </s>\" He's\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = [0] # start with just <s>\n",
    "context = torch.tensor([generated])\n",
    "lang = 1 # French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    langs = torch.zeros_like(context).fill_(lang)\n",
    "    output = model(context, langs=langs)\n",
    "    token = sample_token(output.logits)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = torch.tensor([generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>for the U.S. market. </s>C' est un peu comme les années précédentes, il y a des années. </s>\" Il fallait faire du théâtre. </s>Les prix. </s>La situation est différente. </s>Il a également été un peu plus difficile pour les banques \". </s>Le comité : pour les banques. </s>La commune, le maire, le syndicat des commerçants... </s>A la maison de retraite, les services municipaux... le conseil municipal, la mairie, c' étaient les services de la mairie, qui\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
