{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fb77bdebd30>",
      "text/html": "<style  type=\"text/css\" >\n</style><table id=\"T_3c899_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >id</th>        <th class=\"col_heading level0 col1\" >comment_text</th>        <th class=\"col_heading level0 col2\" >toxic</th>        <th class=\"col_heading level0 col3\" >severe_toxic</th>        <th class=\"col_heading level0 col4\" >obscene</th>        <th class=\"col_heading level0 col5\" >threat</th>        <th class=\"col_heading level0 col6\" >insult</th>        <th class=\"col_heading level0 col7\" >identity_hate</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_3c899_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_3c899_row0_col0\" class=\"data row0 col0\" >0000997932d777bf</td>\n                        <td id=\"T_3c899_row0_col1\" class=\"data row0 col1\" >Explanation\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n                        <td id=\"T_3c899_row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_3c899_row0_col3\" class=\"data row0 col3\" >0</td>\n                        <td id=\"T_3c899_row0_col4\" class=\"data row0 col4\" >0</td>\n                        <td id=\"T_3c899_row0_col5\" class=\"data row0 col5\" >0</td>\n                        <td id=\"T_3c899_row0_col6\" class=\"data row0 col6\" >0</td>\n                        <td id=\"T_3c899_row0_col7\" class=\"data row0 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3c899_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n                        <td id=\"T_3c899_row1_col0\" class=\"data row1 col0\" >000103f0d9cfb60f</td>\n                        <td id=\"T_3c899_row1_col1\" class=\"data row1 col1\" >D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n                        <td id=\"T_3c899_row1_col2\" class=\"data row1 col2\" >0</td>\n                        <td id=\"T_3c899_row1_col3\" class=\"data row1 col3\" >0</td>\n                        <td id=\"T_3c899_row1_col4\" class=\"data row1 col4\" >0</td>\n                        <td id=\"T_3c899_row1_col5\" class=\"data row1 col5\" >0</td>\n                        <td id=\"T_3c899_row1_col6\" class=\"data row1 col6\" >0</td>\n                        <td id=\"T_3c899_row1_col7\" class=\"data row1 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3c899_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n                        <td id=\"T_3c899_row2_col0\" class=\"data row2 col0\" >000113f07ec002fd</td>\n                        <td id=\"T_3c899_row2_col1\" class=\"data row2 col1\" >Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n                        <td id=\"T_3c899_row2_col2\" class=\"data row2 col2\" >0</td>\n                        <td id=\"T_3c899_row2_col3\" class=\"data row2 col3\" >0</td>\n                        <td id=\"T_3c899_row2_col4\" class=\"data row2 col4\" >0</td>\n                        <td id=\"T_3c899_row2_col5\" class=\"data row2 col5\" >0</td>\n                        <td id=\"T_3c899_row2_col6\" class=\"data row2 col6\" >0</td>\n                        <td id=\"T_3c899_row2_col7\" class=\"data row2 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3c899_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n                        <td id=\"T_3c899_row3_col0\" class=\"data row3 col0\" >0001b41b1c6bb37e</td>\n                        <td id=\"T_3c899_row3_col1\" class=\"data row3 col1\" >\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of types of accidents  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport</td>\n                        <td id=\"T_3c899_row3_col2\" class=\"data row3 col2\" >0</td>\n                        <td id=\"T_3c899_row3_col3\" class=\"data row3 col3\" >0</td>\n                        <td id=\"T_3c899_row3_col4\" class=\"data row3 col4\" >0</td>\n                        <td id=\"T_3c899_row3_col5\" class=\"data row3 col5\" >0</td>\n                        <td id=\"T_3c899_row3_col6\" class=\"data row3 col6\" >0</td>\n                        <td id=\"T_3c899_row3_col7\" class=\"data row3 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_3c899_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n                        <td id=\"T_3c899_row4_col0\" class=\"data row4 col0\" >0001d958c54c6e35</td>\n                        <td id=\"T_3c899_row4_col1\" class=\"data row4 col1\" >You, sir, are my hero. Any chance you remember what page that's on?</td>\n                        <td id=\"T_3c899_row4_col2\" class=\"data row4 col2\" >0</td>\n                        <td id=\"T_3c899_row4_col3\" class=\"data row4 col3\" >0</td>\n                        <td id=\"T_3c899_row4_col4\" class=\"data row4 col4\" >0</td>\n                        <td id=\"T_3c899_row4_col5\" class=\"data row4 col5\" >0</td>\n                        <td id=\"T_3c899_row4_col6\" class=\"data row4 col6\" >0</td>\n                        <td id=\"T_3c899_row4_col7\" class=\"data row4 col7\" >0</td>\n            </tr>\n    </tbody></table>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from gensim.models import  doc2vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"./DATA/training.csv\", encoding='ISO8859-2')\n",
    "test = pd.read_csv(\"./DATA/test.csv\", encoding='ISO8859-2')\n",
    "data.head(5).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_all = pd.concat([data, test])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n       'identity_hate'],\n      dtype='object')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_columns = data.columns[2:]\n",
    "y_columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total counts:  233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def remove_strip(str):\n",
    "    return str.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "\n",
    "def get_stop_words(f, encoding='utf-8'):\n",
    "    stop_words = []\n",
    "    with open(f, \"r\", encoding=encoding) as f_stopwords:\n",
    "        for line in f_stopwords:\n",
    "            line = remove_strip(line)\n",
    "            stop_words.append(line.lower())\n",
    "    stop_words = set(stop_words)\n",
    "    print(\"total counts: \", len(stop_words))\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "def read_corpus(fname, tokens_only=False, encoding='utf-8'):\n",
    "    import gensim\n",
    "    import smart_open\n",
    "    with smart_open.open(fname, encoding=encoding) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "stop_words = get_stop_words(\"stopwords.txt\")\n",
    "# data_all['comment_text_clean'].to_csv(\"./DATA/lines.csv\")\n",
    "train_corpus = list(read_corpus(\"./DATA/lines.csv\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(vector_size=20000, window=4, min_count=35, epochs=10, seed=42)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "model.save(fname_or_handle=\"./DATA/doc2vec\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load(fname=\"./DATA/doc2vec\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      0         1         2         3         4         5         6      \\\n0  0.000040  0.000047  0.000040 -0.000038 -0.000042 -0.000023 -0.000021   \n1 -0.000483 -0.000900  0.004402 -0.009081 -0.014446  0.009735 -0.000546   \n2  0.001557  0.007516 -0.001297 -0.003258 -0.006896 -0.002670 -0.001690   \n3 -0.010633  0.008425  0.001052 -0.007180 -0.007521  0.009944 -0.001933   \n4 -0.015639  0.021219  0.012486 -0.019865 -0.003587 -0.005014 -0.014602   \n\n      7         8         9      ...     19990     19991     19992     19993  \\\n0  0.000023 -0.000033 -0.000042  ... -0.000007  0.000049  0.000045 -0.000023   \n1 -0.005104  0.005535 -0.003893  ... -0.010594  0.007578  0.003607  0.005125   \n2 -0.001826  0.004457 -0.002223  ... -0.000198  0.003150 -0.000395  0.002646   \n3  0.004327  0.007524 -0.006032  ... -0.009257 -0.009474 -0.002200  0.008001   \n4  0.015337  0.009624  0.010447  ...  0.013350 -0.007717  0.016971  0.005703   \n\n      19994     19995     19996     19997     19998     19999  \n0  0.000046  0.000003 -0.000012  0.000017 -0.000036 -0.000022  \n1  0.006250  0.001982 -0.003946  0.001351 -0.011819 -0.005357  \n2 -0.002795 -0.003603 -0.002377  0.002033 -0.000099  0.002142  \n3  0.005278  0.010132  0.010924  0.003779 -0.013309 -0.002327  \n4 -0.007462 -0.009086 -0.005975 -0.007747  0.007631 -0.002217  \n\n[5 rows x 20000 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>19990</th>\n      <th>19991</th>\n      <th>19992</th>\n      <th>19993</th>\n      <th>19994</th>\n      <th>19995</th>\n      <th>19996</th>\n      <th>19997</th>\n      <th>19998</th>\n      <th>19999</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000040</td>\n      <td>0.000047</td>\n      <td>0.000040</td>\n      <td>-0.000038</td>\n      <td>-0.000042</td>\n      <td>-0.000023</td>\n      <td>-0.000021</td>\n      <td>0.000023</td>\n      <td>-0.000033</td>\n      <td>-0.000042</td>\n      <td>...</td>\n      <td>-0.000007</td>\n      <td>0.000049</td>\n      <td>0.000045</td>\n      <td>-0.000023</td>\n      <td>0.000046</td>\n      <td>0.000003</td>\n      <td>-0.000012</td>\n      <td>0.000017</td>\n      <td>-0.000036</td>\n      <td>-0.000022</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.000483</td>\n      <td>-0.000900</td>\n      <td>0.004402</td>\n      <td>-0.009081</td>\n      <td>-0.014446</td>\n      <td>0.009735</td>\n      <td>-0.000546</td>\n      <td>-0.005104</td>\n      <td>0.005535</td>\n      <td>-0.003893</td>\n      <td>...</td>\n      <td>-0.010594</td>\n      <td>0.007578</td>\n      <td>0.003607</td>\n      <td>0.005125</td>\n      <td>0.006250</td>\n      <td>0.001982</td>\n      <td>-0.003946</td>\n      <td>0.001351</td>\n      <td>-0.011819</td>\n      <td>-0.005357</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001557</td>\n      <td>0.007516</td>\n      <td>-0.001297</td>\n      <td>-0.003258</td>\n      <td>-0.006896</td>\n      <td>-0.002670</td>\n      <td>-0.001690</td>\n      <td>-0.001826</td>\n      <td>0.004457</td>\n      <td>-0.002223</td>\n      <td>...</td>\n      <td>-0.000198</td>\n      <td>0.003150</td>\n      <td>-0.000395</td>\n      <td>0.002646</td>\n      <td>-0.002795</td>\n      <td>-0.003603</td>\n      <td>-0.002377</td>\n      <td>0.002033</td>\n      <td>-0.000099</td>\n      <td>0.002142</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.010633</td>\n      <td>0.008425</td>\n      <td>0.001052</td>\n      <td>-0.007180</td>\n      <td>-0.007521</td>\n      <td>0.009944</td>\n      <td>-0.001933</td>\n      <td>0.004327</td>\n      <td>0.007524</td>\n      <td>-0.006032</td>\n      <td>...</td>\n      <td>-0.009257</td>\n      <td>-0.009474</td>\n      <td>-0.002200</td>\n      <td>0.008001</td>\n      <td>0.005278</td>\n      <td>0.010132</td>\n      <td>0.010924</td>\n      <td>0.003779</td>\n      <td>-0.013309</td>\n      <td>-0.002327</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.015639</td>\n      <td>0.021219</td>\n      <td>0.012486</td>\n      <td>-0.019865</td>\n      <td>-0.003587</td>\n      <td>-0.005014</td>\n      <td>-0.014602</td>\n      <td>0.015337</td>\n      <td>0.009624</td>\n      <td>0.010447</td>\n      <td>...</td>\n      <td>0.013350</td>\n      <td>-0.007717</td>\n      <td>0.016971</td>\n      <td>0.005703</td>\n      <td>-0.007462</td>\n      <td>-0.009086</td>\n      <td>-0.005975</td>\n      <td>-0.007747</td>\n      <td>0.007631</td>\n      <td>-0.002217</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 20000 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc2vec = model.dv.vectors[:127657]\n",
    "test_doc2vec = model.dv.vectors[127657:]\n",
    "\n",
    "train_df = pd.DataFrame(train_doc2vec)\n",
    "train_df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "del train_doc2vec\n",
    "del model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = data['toxic']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "              0         1         2         3         4         5         6   \\\n0       0.000040  0.000047  0.000040 -0.000038 -0.000042 -0.000023 -0.000021   \n1      -0.000483 -0.000900  0.004402 -0.009081 -0.014446  0.009735 -0.000546   \n2       0.001557  0.007516 -0.001297 -0.003258 -0.006896 -0.002670 -0.001690   \n3      -0.010633  0.008425  0.001052 -0.007180 -0.007521  0.009944 -0.001933   \n4      -0.015639  0.021219  0.012486 -0.019865 -0.003587 -0.005014 -0.014602   \n...          ...       ...       ...       ...       ...       ...       ...   \n127652 -0.000555  0.009785 -0.001923 -0.009518 -0.007267  0.005526 -0.012722   \n127653  0.002812  0.024820 -0.033996  0.015426 -0.012184  0.000426 -0.001530   \n127654 -0.004708  0.009964 -0.003932 -0.000361 -0.002112  0.002400 -0.000918   \n127655 -0.004697  0.005901 -0.000093  0.003511 -0.003059  0.001944  0.003853   \n127656 -0.011279  0.002261 -0.000589 -0.000629 -0.000362  0.002485  0.002952   \n\n              7         8         9   ...        11        12        13  \\\n0       0.000023 -0.000033 -0.000042  ...  0.000049 -0.000023  0.000007   \n1      -0.005104  0.005535 -0.003893  ...  0.008390 -0.002746 -0.007831   \n2      -0.001826  0.004457 -0.002223  ...  0.000078  0.000650  0.001475   \n3       0.004327  0.007524 -0.006032  ... -0.001479 -0.006915  0.007778   \n4       0.015337  0.009624  0.010447  ...  0.013868  0.010003  0.000679   \n...          ...       ...       ...  ...       ...       ...       ...   \n127652 -0.002927  0.028967  0.016880  ...  0.004161 -0.011312  0.000027   \n127653 -0.004660  0.024888  0.005545  ...  0.008329  0.000550 -0.013695   \n127654  0.001660  0.004699  0.008152  ...  0.004081 -0.010665  0.001971   \n127655  0.003096  0.002653 -0.000658  ... -0.001412 -0.002119 -0.000663   \n127656 -0.002506 -0.006281  0.005632  ...  0.005719 -0.001548 -0.002760   \n\n              14        15        16        17        18        19        20  \n0       0.000044  0.000022 -0.000005  0.000047 -0.000030  0.000028  0.000020  \n1      -0.003512  0.003077  0.006789 -0.005752  0.004813 -0.001654  0.006974  \n2      -0.003079 -0.003559 -0.001850 -0.003219 -0.003403  0.006526 -0.000830  \n3       0.011296  0.001880 -0.006356 -0.005767  0.005237  0.000480  0.001613  \n4       0.007855 -0.009814 -0.003690 -0.017954  0.005277 -0.004219 -0.003766  \n...          ...       ...       ...       ...       ...       ...       ...  \n127652  0.007001  0.010018  0.001193 -0.003227 -0.008497 -0.009833 -0.003798  \n127653 -0.001621 -0.005426 -0.004614 -0.010515 -0.015325 -0.028933 -0.013656  \n127654  0.002411 -0.000859 -0.002428  0.000664 -0.004512 -0.001895  0.003898  \n127655  0.000381 -0.001680 -0.002318 -0.002244 -0.001480  0.001142 -0.001697  \n127656  0.003530 -0.000611 -0.003481  0.006302  0.002305 -0.001645 -0.000561  \n\n[127657 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000040</td>\n      <td>0.000047</td>\n      <td>0.000040</td>\n      <td>-0.000038</td>\n      <td>-0.000042</td>\n      <td>-0.000023</td>\n      <td>-0.000021</td>\n      <td>0.000023</td>\n      <td>-0.000033</td>\n      <td>-0.000042</td>\n      <td>...</td>\n      <td>0.000049</td>\n      <td>-0.000023</td>\n      <td>0.000007</td>\n      <td>0.000044</td>\n      <td>0.000022</td>\n      <td>-0.000005</td>\n      <td>0.000047</td>\n      <td>-0.000030</td>\n      <td>0.000028</td>\n      <td>0.000020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.000483</td>\n      <td>-0.000900</td>\n      <td>0.004402</td>\n      <td>-0.009081</td>\n      <td>-0.014446</td>\n      <td>0.009735</td>\n      <td>-0.000546</td>\n      <td>-0.005104</td>\n      <td>0.005535</td>\n      <td>-0.003893</td>\n      <td>...</td>\n      <td>0.008390</td>\n      <td>-0.002746</td>\n      <td>-0.007831</td>\n      <td>-0.003512</td>\n      <td>0.003077</td>\n      <td>0.006789</td>\n      <td>-0.005752</td>\n      <td>0.004813</td>\n      <td>-0.001654</td>\n      <td>0.006974</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001557</td>\n      <td>0.007516</td>\n      <td>-0.001297</td>\n      <td>-0.003258</td>\n      <td>-0.006896</td>\n      <td>-0.002670</td>\n      <td>-0.001690</td>\n      <td>-0.001826</td>\n      <td>0.004457</td>\n      <td>-0.002223</td>\n      <td>...</td>\n      <td>0.000078</td>\n      <td>0.000650</td>\n      <td>0.001475</td>\n      <td>-0.003079</td>\n      <td>-0.003559</td>\n      <td>-0.001850</td>\n      <td>-0.003219</td>\n      <td>-0.003403</td>\n      <td>0.006526</td>\n      <td>-0.000830</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.010633</td>\n      <td>0.008425</td>\n      <td>0.001052</td>\n      <td>-0.007180</td>\n      <td>-0.007521</td>\n      <td>0.009944</td>\n      <td>-0.001933</td>\n      <td>0.004327</td>\n      <td>0.007524</td>\n      <td>-0.006032</td>\n      <td>...</td>\n      <td>-0.001479</td>\n      <td>-0.006915</td>\n      <td>0.007778</td>\n      <td>0.011296</td>\n      <td>0.001880</td>\n      <td>-0.006356</td>\n      <td>-0.005767</td>\n      <td>0.005237</td>\n      <td>0.000480</td>\n      <td>0.001613</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.015639</td>\n      <td>0.021219</td>\n      <td>0.012486</td>\n      <td>-0.019865</td>\n      <td>-0.003587</td>\n      <td>-0.005014</td>\n      <td>-0.014602</td>\n      <td>0.015337</td>\n      <td>0.009624</td>\n      <td>0.010447</td>\n      <td>...</td>\n      <td>0.013868</td>\n      <td>0.010003</td>\n      <td>0.000679</td>\n      <td>0.007855</td>\n      <td>-0.009814</td>\n      <td>-0.003690</td>\n      <td>-0.017954</td>\n      <td>0.005277</td>\n      <td>-0.004219</td>\n      <td>-0.003766</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>127652</th>\n      <td>-0.000555</td>\n      <td>0.009785</td>\n      <td>-0.001923</td>\n      <td>-0.009518</td>\n      <td>-0.007267</td>\n      <td>0.005526</td>\n      <td>-0.012722</td>\n      <td>-0.002927</td>\n      <td>0.028967</td>\n      <td>0.016880</td>\n      <td>...</td>\n      <td>0.004161</td>\n      <td>-0.011312</td>\n      <td>0.000027</td>\n      <td>0.007001</td>\n      <td>0.010018</td>\n      <td>0.001193</td>\n      <td>-0.003227</td>\n      <td>-0.008497</td>\n      <td>-0.009833</td>\n      <td>-0.003798</td>\n    </tr>\n    <tr>\n      <th>127653</th>\n      <td>0.002812</td>\n      <td>0.024820</td>\n      <td>-0.033996</td>\n      <td>0.015426</td>\n      <td>-0.012184</td>\n      <td>0.000426</td>\n      <td>-0.001530</td>\n      <td>-0.004660</td>\n      <td>0.024888</td>\n      <td>0.005545</td>\n      <td>...</td>\n      <td>0.008329</td>\n      <td>0.000550</td>\n      <td>-0.013695</td>\n      <td>-0.001621</td>\n      <td>-0.005426</td>\n      <td>-0.004614</td>\n      <td>-0.010515</td>\n      <td>-0.015325</td>\n      <td>-0.028933</td>\n      <td>-0.013656</td>\n    </tr>\n    <tr>\n      <th>127654</th>\n      <td>-0.004708</td>\n      <td>0.009964</td>\n      <td>-0.003932</td>\n      <td>-0.000361</td>\n      <td>-0.002112</td>\n      <td>0.002400</td>\n      <td>-0.000918</td>\n      <td>0.001660</td>\n      <td>0.004699</td>\n      <td>0.008152</td>\n      <td>...</td>\n      <td>0.004081</td>\n      <td>-0.010665</td>\n      <td>0.001971</td>\n      <td>0.002411</td>\n      <td>-0.000859</td>\n      <td>-0.002428</td>\n      <td>0.000664</td>\n      <td>-0.004512</td>\n      <td>-0.001895</td>\n      <td>0.003898</td>\n    </tr>\n    <tr>\n      <th>127655</th>\n      <td>-0.004697</td>\n      <td>0.005901</td>\n      <td>-0.000093</td>\n      <td>0.003511</td>\n      <td>-0.003059</td>\n      <td>0.001944</td>\n      <td>0.003853</td>\n      <td>0.003096</td>\n      <td>0.002653</td>\n      <td>-0.000658</td>\n      <td>...</td>\n      <td>-0.001412</td>\n      <td>-0.002119</td>\n      <td>-0.000663</td>\n      <td>0.000381</td>\n      <td>-0.001680</td>\n      <td>-0.002318</td>\n      <td>-0.002244</td>\n      <td>-0.001480</td>\n      <td>0.001142</td>\n      <td>-0.001697</td>\n    </tr>\n    <tr>\n      <th>127656</th>\n      <td>-0.011279</td>\n      <td>0.002261</td>\n      <td>-0.000589</td>\n      <td>-0.000629</td>\n      <td>-0.000362</td>\n      <td>0.002485</td>\n      <td>0.002952</td>\n      <td>-0.002506</td>\n      <td>-0.006281</td>\n      <td>0.005632</td>\n      <td>...</td>\n      <td>0.005719</td>\n      <td>-0.001548</td>\n      <td>-0.002760</td>\n      <td>0.003530</td>\n      <td>-0.000611</td>\n      <td>-0.003481</td>\n      <td>0.006302</td>\n      <td>0.002305</td>\n      <td>-0.001645</td>\n      <td>-0.000561</td>\n    </tr>\n  </tbody>\n</table>\n<p>127657 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[:, 0:20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df.loc[:, :1000], y, test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "LogisticRegression(C=3.0, random_state=42, solver='sag')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=42)\n",
    "classifier.fit(train_df[:60000], y[:60000])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-d41ca20d89eb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mf1_score\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'toxic-训练集F1值:%s'\u001B[0m\u001B[0;34m%\u001B[0m\u001B[0mf1_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m60000\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m60000\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'toxic-验证集F1值:%s'\u001B[0m\u001B[0;34m%\u001B[0m\u001B[0mf1_score\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_df\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m60000\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m60000\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36mf1_score\u001B[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[1;32m   1069\u001B[0m     \u001B[0mmodified\u001B[0m \u001B[0;32mwith\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mzero_division\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1070\u001B[0m     \"\"\"\n\u001B[0;32m-> 1071\u001B[0;31m     return fbeta_score(y_true, y_pred, beta=1, labels=labels,\n\u001B[0m\u001B[1;32m   1072\u001B[0m                        \u001B[0mpos_label\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpos_label\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maverage\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maverage\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1073\u001B[0m                        \u001B[0msample_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36mfbeta_score\u001B[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[1;32m   1193\u001B[0m     \"\"\"\n\u001B[1;32m   1194\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1195\u001B[0;31m     _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001B[0m\u001B[1;32m   1196\u001B[0m                                                  \u001B[0mbeta\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbeta\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1197\u001B[0m                                                  \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36minner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[0;34m<=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;31m# extra_args > 0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36mprecision_recall_fscore_support\u001B[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[0m\n\u001B[1;32m   1462\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mbeta\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1463\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"beta should be >=0 in the F-beta score\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1464\u001B[0;31m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001B[0m\u001B[1;32m   1465\u001B[0m                                     pos_label)\n\u001B[1;32m   1466\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36m_check_set_wise_labels\u001B[0;34m(y_true, y_pred, average, labels, pos_label)\u001B[0m\n\u001B[1;32m   1275\u001B[0m                          str(average_options))\n\u001B[1;32m   1276\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1277\u001B[0;31m     \u001B[0my_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_check_targets\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1278\u001B[0m     \u001B[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1279\u001B[0m     \u001B[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001B[0m in \u001B[0;36m_check_targets\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_type\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001B[0m\u001B[1;32m     93\u001B[0m                          \"and {1} targets\".format(type_true, type_pred))\n\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Classification metrics can't handle a mix of continuous-multioutput and binary targets"
     ]
    }
   ],
   "source": [
    "pred = classifier.predict(train_df[60000:])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('toxic-训练集F1值:%s'%f1_score(train_df[:60000], y[:60000]))\n",
    "print('toxic-验证集F1值:%s'%f1_score(train_df[60000:], y[60000:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=42)\n",
    "classifier.fit(X_over, y_over)\n",
    "train_pred_toxic = classifier.predict(X_train)\n",
    "test_pred_toxic = classifier.predict(X_test)\n",
    "predict_toxic_0 = classifier.predict(test_doc2vec)\n",
    "predict_toxic=pd.DataFrame(predict_toxic_0,columns=['toxic'])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('toxic-训练集F1值:%s'%f1_score(train_pred_toxic,y_train))\n",
    "print('toxic-验证集F1值:%s'%f1_score(test_pred_toxic,y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(24364, 300)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "models = []\n",
    "i = 0\n",
    "test_size = 0.25\n",
    "rus = RandomUnderSampler(random_state=random_seed, )\n",
    "X, y= rus.fit_resample(data_doc2vec, data[y_columns[i]])\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set f1 score:  0.9697877118870648\n",
      "training set toxic f1_score: 76.98\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "lgb =LGBMClassifier(max_depth=8, num_leaves=200, boosting_type='dart', random_state=random_seed)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_train_pred = lgb.predict(X_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "print(\"train set f1 score: \", f1_score(y_train, y_train_pred))\n",
    "print(\"training set %s f1_score: %.2f\" % (y_columns[i], f1_score(y_valid, y_valid_pred) * 100))\n",
    "models.append(lgb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-52a413d9",
   "language": "python",
   "display_name": "PyCharm (TwilightStruggle)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}