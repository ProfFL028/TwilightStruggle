{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\proffl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download([\n",
    "    \"names\",\n",
    "    \"stopwords\",\n",
    "    \"state_union\",\n",
    "    \"twitter_samples\",\n",
    "    \"movie_reviews\",\n",
    "    \"averaged_perceptron_tagger\",\n",
    "    \"vader_lexicon\",\n",
    "    \"punkt\",\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                      id                                       comment_text  \\\n0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3       0001b41b1c6bb37e  \\nMore\\nI can't make any real suggestions on i...   \n4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n...                  ...                                                ...   \n127652  ffe8b9316245be30  The numbers in parentheses are the additional ...   \n127653  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n127654  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n127655  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n127656  fff46fc426af1f9a  \\nAnd ... I really don't think you understand....   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n0           0             0        0       0       0              0  \n1           0             0        0       0       0              0  \n2           0             0        0       0       0              0  \n3           0             0        0       0       0              0  \n4           0             0        0       0       0              0  \n...       ...           ...      ...     ...     ...            ...  \n127652      0             0        0       0       0              0  \n127653      0             0        0       0       0              0  \n127654      0             0        0       0       0              0  \n127655      0             0        0       0       0              0  \n127656      0             0        0       0       0              0  \n\n[127657 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\\nMore\\nI can't make any real suggestions on i...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>127652</th>\n      <td>ffe8b9316245be30</td>\n      <td>The numbers in parentheses are the additional ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>127653</th>\n      <td>ffea4adeee384e90</td>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>127654</th>\n      <td>ffee36eab5c267c9</td>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>127655</th>\n      <td>fff125370e4aaaf3</td>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>127656</th>\n      <td>fff46fc426af1f9a</td>\n      <td>\\nAnd ... I really don't think you understand....</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>127657 rows Ã— 8 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./DATA/training.csv\", encoding='ISO8859-2')\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "0         Explanation edits made username Hardcore Metal...\n1         D'aww! matches background colour I'm seemingly...\n2         Hey man, I'm really trying edit war. guy const...\n3         can't make real suggestions improvement - wond...\n4           You, sir, hero. chance remember page that's on?\n                                ...                        \n127652    numbers parentheses additional decimal points ...\n127653    ashamed horrible thing put talk page. 128.61.1...\n127654    Spitzer Umm, theres actual article prostitutio...\n127655    looks like actually put speedy first version d...\n127656    ... really think understand. came idea bad rig...\nName: comment_text_sw, Length: 127657, dtype: object"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "data['comment_text_sw'] = data['comment_text'].apply(\n",
    "    lambda text: \" \".join([w for w in text.split() if w.lower() not in stopwords]))\n",
    "data['comment_text_sw']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-23-3076ea64e982>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msentiment\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSentimentIntensityAnalyzer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0msia\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSentimentIntensityAnalyzer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'emotion'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'comment_text'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msia\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolarity_scores\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'emotion'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4355\u001B[0m         \u001B[0mdtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mfloat64\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4356\u001B[0m         \"\"\"\n\u001B[1;32m-> 4357\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mSeriesApply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconvert_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4358\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4359\u001B[0m     def _reduce(\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1041\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1042\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1043\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_standard\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1044\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1045\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0magg\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001B[0m in \u001B[0;36mapply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1096\u001B[0m                 \u001B[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1097\u001B[0m                 \u001B[1;31m# \"Callable[[Any], Any]\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1098\u001B[1;33m                 mapped = lib.map_infer(\n\u001B[0m\u001B[0;32m   1099\u001B[0m                     \u001B[0mvalues\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1100\u001B[0m                     \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# type: ignore[arg-type]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001B[0m in \u001B[0;36mpolarity_scores\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    358\u001B[0m         \"\"\"\n\u001B[0;32m    359\u001B[0m         \u001B[1;31m# text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 360\u001B[1;33m         sentitext = SentiText(text, self.constants.PUNC_LIST,\n\u001B[0m\u001B[0;32m    361\u001B[0m                               self.constants.REGEX_REMOVE_PUNCTUATION)\n\u001B[0;32m    362\u001B[0m         \u001B[0msentiments\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, text, punc_list, regex_remove_punctuation)\u001B[0m\n\u001B[0;32m    272\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPUNC_LIST\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpunc_list\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    273\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mREGEX_REMOVE_PUNCTUATION\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mregex_remove_punctuation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 274\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwords_and_emoticons\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_words_and_emoticons\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    275\u001B[0m         \u001B[1;31m# doesn't separate words from\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    276\u001B[0m         \u001B[1;31m# adjacent punctuation (keeps emoticons & contractions)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001B[0m in \u001B[0;36m_words_and_emoticons\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    304\u001B[0m         \"\"\"\n\u001B[0;32m    305\u001B[0m         \u001B[0mwes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 306\u001B[1;33m         \u001B[0mwords_punc_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_words_plus_punc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    307\u001B[0m         \u001B[0mwes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mwe\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mwe\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mwes\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwe\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwe\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001B[0m in \u001B[0;36m_words_plus_punc\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    292\u001B[0m         \u001B[1;31m# the product gives ('cat', ',') and (',', 'cat')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m         \u001B[0mpunc_before\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mproduct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPUNC_LIST\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwords_only\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 294\u001B[1;33m         \u001B[0mpunc_after\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mproduct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwords_only\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPUNC_LIST\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    295\u001B[0m         \u001B[0mwords_punc_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpunc_before\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    296\u001B[0m         \u001B[0mwords_punc_dict\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpunc_after\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001B[0m in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    292\u001B[0m         \u001B[1;31m# the product gives ('cat', ',') and (',', 'cat')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    293\u001B[0m         \u001B[0mpunc_before\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mproduct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPUNC_LIST\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwords_only\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 294\u001B[1;33m         \u001B[0mpunc_after\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m\"\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mp\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mproduct\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwords_only\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPUNC_LIST\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    295\u001B[0m         \u001B[0mwords_punc_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpunc_before\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    296\u001B[0m         \u001B[0mwords_punc_dict\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpunc_after\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "data['emotion'] = data['comment_text'].apply(sia.polarity_scores)\n",
    "data.loc[0, 'emotion']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "{'neg': 0.0, 'neu': 0.897, 'pos': 0.103, 'compound': 0.5574}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[0, 'emotion']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "                      id                                       comment_text  \\\n0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n3       0001b41b1c6bb37e  \\nMore\\nI can't make any real suggestions on i...   \n4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n...                  ...                                                ...   \n127652  ffe8b9316245be30  The numbers in parentheses are the additional ...   \n127653  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n127654  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n127655  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n127656  fff46fc426af1f9a  \\nAnd ... I really don't think you understand....   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n0           0             0        0       0       0              0   \n1           0             0        0       0       0              0   \n2           0             0        0       0       0              0   \n3           0             0        0       0       0              0   \n4           0             0        0       0       0              0   \n...       ...           ...      ...     ...     ...            ...   \n127652      0             0        0       0       0              0   \n127653      0             0        0       0       0              0   \n127654      0             0        0       0       0              0   \n127655      0             0        0       0       0              0   \n127656      0             0        0       0       0              0   \n\n                                          comment_text_sw  \\\n0       [Explanation, edits, made, username, Hardcore,...   \n1       [D'aww!, matches, background, colour, I'm, see...   \n2       [Hey, man,, I'm, really, trying, edit, war., g...   \n3       [can't, make, real, suggestions, improvement, ...   \n4       [You,, sir,, hero., chance, remember, page, th...   \n...                                                   ...   \n127652  [numbers, parentheses, additional, decimal, po...   \n127653  [ashamed, horrible, thing, put, talk, page., 1...   \n127654  [Spitzer, Umm,, theres, actual, article, prost...   \n127655  [looks, like, actually, put, speedy, first, ve...   \n127656  [..., really, think, understand., came, idea, ...   \n\n                                    comment_text_tokenize  \\\n0       [Explanation, Why, the, edits, made, under, my...   \n1       [D'aww, !, He, matches, this, background, colo...   \n2       [Hey, man, ,, I, 'm, really, not, trying, to, ...   \n3       [More, I, ca, n't, make, any, real, suggestion...   \n4       [You, ,, sir, ,, are, my, hero, ., Any, chance...   \n...                                                   ...   \n127652  [The, numbers, in, parentheses, are, the, addi...   \n127653  [You, should, be, ashamed, of, yourself, That,...   \n127654  [Spitzer, Umm, ,, theres, no, actual, article,...   \n127655  [And, it, looks, like, it, was, actually, you,...   \n127656  [And, ..., I, really, do, n't, think, you, und...   \n\n                                                  emotion  emotion_neg  \\\n0       {'neg': 0.0, 'neu': 0.897, 'pos': 0.103, 'comp...        0.000   \n1       {'neg': 0.099, 'neu': 0.743, 'pos': 0.158, 'co...        0.099   \n2       {'neg': 0.083, 'neu': 0.849, 'pos': 0.068, 'co...        0.083   \n3       {'neg': 0.043, 'neu': 0.896, 'pos': 0.062, 'co...        0.043   \n4       {'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'comp...        0.000   \n...                                                   ...          ...   \n127652  {'neg': 0.068, 'neu': 0.75, 'pos': 0.183, 'com...        0.068   \n127653  {'neg': 0.306, 'neu': 0.694, 'pos': 0.0, 'comp...        0.306   \n127654  {'neg': 0.18, 'neu': 0.82, 'pos': 0.0, 'compou...        0.180   \n127655  {'neg': 0.0, 'neu': 0.902, 'pos': 0.098, 'comp...        0.000   \n127656  {'neg': 0.183, 'neu': 0.759, 'pos': 0.058, 'co...        0.183   \n\n        emotion_neu  emotion_pos  emotion_compound  \n0             0.897        0.103            0.5574  \n1             0.743        0.158            0.2942  \n2             0.849        0.068           -0.1779  \n3             0.896        0.062            0.2500  \n4             0.663        0.337            0.6808  \n...             ...          ...               ...  \n127652        0.750        0.183            0.9331  \n127653        0.694        0.000           -0.7650  \n127654        0.820        0.000           -0.2960  \n127655        0.902        0.098            0.3612  \n127656        0.759        0.058           -0.7003  \n\n[127657 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n      <th>comment_text_sw</th>\n      <th>comment_text_tokenize</th>\n      <th>emotion</th>\n      <th>emotion_neg</th>\n      <th>emotion_neu</th>\n      <th>emotion_pos</th>\n      <th>emotion_compound</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[Explanation, edits, made, username, Hardcore,...</td>\n      <td>[Explanation, Why, the, edits, made, under, my...</td>\n      <td>{'neg': 0.0, 'neu': 0.897, 'pos': 0.103, 'comp...</td>\n      <td>0.000</td>\n      <td>0.897</td>\n      <td>0.103</td>\n      <td>0.5574</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[D'aww!, matches, background, colour, I'm, see...</td>\n      <td>[D'aww, !, He, matches, this, background, colo...</td>\n      <td>{'neg': 0.099, 'neu': 0.743, 'pos': 0.158, 'co...</td>\n      <td>0.099</td>\n      <td>0.743</td>\n      <td>0.158</td>\n      <td>0.2942</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[Hey, man,, I'm, really, trying, edit, war., g...</td>\n      <td>[Hey, man, ,, I, 'm, really, not, trying, to, ...</td>\n      <td>{'neg': 0.083, 'neu': 0.849, 'pos': 0.068, 'co...</td>\n      <td>0.083</td>\n      <td>0.849</td>\n      <td>0.068</td>\n      <td>-0.1779</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\\nMore\\nI can't make any real suggestions on i...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[can't, make, real, suggestions, improvement, ...</td>\n      <td>[More, I, ca, n't, make, any, real, suggestion...</td>\n      <td>{'neg': 0.043, 'neu': 0.896, 'pos': 0.062, 'co...</td>\n      <td>0.043</td>\n      <td>0.896</td>\n      <td>0.062</td>\n      <td>0.2500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[You,, sir,, hero., chance, remember, page, th...</td>\n      <td>[You, ,, sir, ,, are, my, hero, ., Any, chance...</td>\n      <td>{'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'comp...</td>\n      <td>0.000</td>\n      <td>0.663</td>\n      <td>0.337</td>\n      <td>0.6808</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>127652</th>\n      <td>ffe8b9316245be30</td>\n      <td>The numbers in parentheses are the additional ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[numbers, parentheses, additional, decimal, po...</td>\n      <td>[The, numbers, in, parentheses, are, the, addi...</td>\n      <td>{'neg': 0.068, 'neu': 0.75, 'pos': 0.183, 'com...</td>\n      <td>0.068</td>\n      <td>0.750</td>\n      <td>0.183</td>\n      <td>0.9331</td>\n    </tr>\n    <tr>\n      <th>127653</th>\n      <td>ffea4adeee384e90</td>\n      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[ashamed, horrible, thing, put, talk, page., 1...</td>\n      <td>[You, should, be, ashamed, of, yourself, That,...</td>\n      <td>{'neg': 0.306, 'neu': 0.694, 'pos': 0.0, 'comp...</td>\n      <td>0.306</td>\n      <td>0.694</td>\n      <td>0.000</td>\n      <td>-0.7650</td>\n    </tr>\n    <tr>\n      <th>127654</th>\n      <td>ffee36eab5c267c9</td>\n      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[Spitzer, Umm,, theres, actual, article, prost...</td>\n      <td>[Spitzer, Umm, ,, theres, no, actual, article,...</td>\n      <td>{'neg': 0.18, 'neu': 0.82, 'pos': 0.0, 'compou...</td>\n      <td>0.180</td>\n      <td>0.820</td>\n      <td>0.000</td>\n      <td>-0.2960</td>\n    </tr>\n    <tr>\n      <th>127655</th>\n      <td>fff125370e4aaaf3</td>\n      <td>And it looks like it was actually you who put ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[looks, like, actually, put, speedy, first, ve...</td>\n      <td>[And, it, looks, like, it, was, actually, you,...</td>\n      <td>{'neg': 0.0, 'neu': 0.902, 'pos': 0.098, 'comp...</td>\n      <td>0.000</td>\n      <td>0.902</td>\n      <td>0.098</td>\n      <td>0.3612</td>\n    </tr>\n    <tr>\n      <th>127656</th>\n      <td>fff46fc426af1f9a</td>\n      <td>\\nAnd ... I really don't think you understand....</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[..., really, think, understand., came, idea, ...</td>\n      <td>[And, ..., I, really, do, n't, think, you, und...</td>\n      <td>{'neg': 0.183, 'neu': 0.759, 'pos': 0.058, 'co...</td>\n      <td>0.183</td>\n      <td>0.759</td>\n      <td>0.058</td>\n      <td>-0.7003</td>\n    </tr>\n  </tbody>\n</table>\n<p>127657 rows Ã— 15 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['emotion_neg'] = data['emotion'].apply(lambda x: x['neg'])\n",
    "data['emotion_neu'] = data['emotion'].apply(lambda x: x['neu'])\n",
    "data['emotion_pos'] = data['emotion'].apply(lambda x: x['pos'])\n",
    "data['emotion_compound'] = data['emotion'].apply(lambda x: x['compound'])\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "X = data[['emotion_neg', 'emotion_neu', 'emotion_pos', 'emotion_compound']]\n",
    "y_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n",
    "             'identity_hate']\n",
    "models = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2242, 127657]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-101-0a4d450a5b07>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0my_column\u001B[0m \u001B[1;32min\u001B[0m \u001B[0my_columns\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0my_column\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_valid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_valid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.25\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstratify\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[0mlightgbm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msklearn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLGBMClassifier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mf1_score\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001B[0m in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2417\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"At least one array required as input\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2418\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2419\u001B[1;33m     \u001B[0marrays\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mindexable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2420\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2421\u001B[0m     \u001B[0mn_samples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_num_samples\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    368\u001B[0m     \"\"\"\n\u001B[0;32m    369\u001B[0m     \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0m_make_indexable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mX\u001B[0m \u001B[1;32min\u001B[0m \u001B[0miterables\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 370\u001B[1;33m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    371\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    372\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    329\u001B[0m     \u001B[0muniques\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlengths\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muniques\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 331\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m    332\u001B[0m             \u001B[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m             \u001B[1;33m%\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ml\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlengths\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [2242, 127657]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for y_column in y_columns:\n",
    "    y = data[y_column]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "    from lightgbm.sklearn import LGBMClassifier\n",
    "    from sklearn.metrics import f1_score\n",
    "    lgb =LGBMClassifier(max_depth=7, num_leaves=200,boosting_type='dart')\n",
    "    lgb.fit(X_train, y_train)\n",
    "    y_valid_pred = lgb.predict(X_valid)\n",
    "    models.append(lgb)\n",
    "    print(\"%s f1_score: %.2f\" % (y_column, f1_score(y_valid, y_valid_pred) * 100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "33        [You, are, gay, or, antisemmitian?, Archangel,...\n88        [A, pair, of, jew-hating, weiner, nazi, schmuc...\n146       [I, think, that, your, a, Fagget, get, a, oife...\n180       [Kill, all, niggers., I, have, hard,, that, ot...\n193       [u, r, a, tw@, fuck, off, u, gay, boy.U, r, sm...\n                                ...                        \n127394    [whats, up, gook, Whats, up, you, asian, gook,...\n127428    [LoL!!, you're, GAY!!, you, will, never, know,...\n127473    [Oh,, fuck, off., The, pansy, Jew, would, just...\n127525    [Shalom, Semite,, get, the, fuck, out, of, her...\n127598    [our, previous, conversation, you, fucking, sh...\nName: comment_text_split, Length: 1121, dtype: object"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hate = data.loc[data['identity_hate'] == 1, ['comment_text']]\n",
    "data_hate['comment_text_split'] = data_hate['comment_text'].apply(lambda text: [w for w in text.split()])\n",
    "data_hate['comment_text_split']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "                                        comment_text_sw\n0     Section break Let state fact: think Religious ...\n1                    first experience typing Wikipedia.\n2     support Jackie !! right thing. truth facts cor...\n3     Season Five Tables Since half cast confirmed c...\n4            Article article circus, every day changes.\n...                                                 ...\n2237  whats gook Whats asian gook chink man hoe chow...\n2238  LoL!! GAY!! never know good feels fuck woman a...\n2239  Oh, fuck off. pansy Jew would whine B'nai Brit...\n2240  Shalom Semite, get fuck here. kill you, son bi...\n2241  previous conversation fucking shit eating libe...\n\n[2242 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text_sw</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Section break Let state fact: think Religious ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>first experience typing Wikipedia.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>support Jackie !! right thing. truth facts cor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Season Five Tables Since half cast confirmed c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Article article circus, every day changes.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2237</th>\n      <td>whats gook Whats asian gook chink man hoe chow...</td>\n    </tr>\n    <tr>\n      <th>2238</th>\n      <td>LoL!! GAY!! never know good feels fuck woman a...</td>\n    </tr>\n    <tr>\n      <th>2239</th>\n      <td>Oh, fuck off. pansy Jew would whine B'nai Brit...</td>\n    </tr>\n    <tr>\n      <th>2240</th>\n      <td>Shalom Semite, get fuck here. kill you, son bi...</td>\n    </tr>\n    <tr>\n      <th>2241</th>\n      <td>previous conversation fucking shit eating libe...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2242 rows Ã— 1 columns</p>\n</div>"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "data_some =data[['comment_text', 'identity_hate']]\n",
    "rus = RandomUnderSampler()\n",
    "X, y= rus.fit_resample(data[['comment_text_sw']], data[['identity_hate']])\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2242, 127657]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-103-0857b3ed83c9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mx_pca\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_pca\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_valid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_valid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_pca\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.25\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstratify\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mlightgbm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msklearn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLGBMClassifier\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmetrics\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mf1_score\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001B[0m in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2417\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"At least one array required as input\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2418\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2419\u001B[1;33m     \u001B[0marrays\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mindexable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2420\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2421\u001B[0m     \u001B[0mn_samples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_num_samples\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrays\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mindexable\u001B[1;34m(*iterables)\u001B[0m\n\u001B[0;32m    368\u001B[0m     \"\"\"\n\u001B[0;32m    369\u001B[0m     \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0m_make_indexable\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mX\u001B[0m \u001B[1;32min\u001B[0m \u001B[0miterables\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 370\u001B[1;33m     \u001B[0mcheck_consistent_length\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    371\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    372\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_consistent_length\u001B[1;34m(*arrays)\u001B[0m\n\u001B[0;32m    329\u001B[0m     \u001B[0muniques\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlengths\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    330\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0muniques\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 331\u001B[1;33m         raise ValueError(\n\u001B[0m\u001B[0;32m    332\u001B[0m             \u001B[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m             \u001B[1;33m%\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ml\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ml\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlengths\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [2242, 127657]"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "X_tfidf = tfidf.fit_transform(X['comment_text_sw'])\n",
    "from sklearn.decomposition import PCA\n",
    "X_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(X_df)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_pca, y, test_size=0.25, stratify=y)\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "lgb =LGBMClassifier()\n",
    "lgb.fit(X_train, y_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "models.append(lgb)\n",
    "print(\"%s f1_score: %.2f\" % (y_column, f1_score(y_valid, y_valid_pred) * 100))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}