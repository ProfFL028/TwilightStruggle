{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fa10143bb50>",
      "text/html": "<style  type=\"text/css\" >\n</style><table id=\"T_77656_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >id</th>        <th class=\"col_heading level0 col1\" >comment_text</th>        <th class=\"col_heading level0 col2\" >toxic</th>        <th class=\"col_heading level0 col3\" >severe_toxic</th>        <th class=\"col_heading level0 col4\" >obscene</th>        <th class=\"col_heading level0 col5\" >threat</th>        <th class=\"col_heading level0 col6\" >insult</th>        <th class=\"col_heading level0 col7\" >identity_hate</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_77656_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_77656_row0_col0\" class=\"data row0 col0\" >0000997932d777bf</td>\n                        <td id=\"T_77656_row0_col1\" class=\"data row0 col1\" >Explanation\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n                        <td id=\"T_77656_row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_77656_row0_col3\" class=\"data row0 col3\" >0</td>\n                        <td id=\"T_77656_row0_col4\" class=\"data row0 col4\" >0</td>\n                        <td id=\"T_77656_row0_col5\" class=\"data row0 col5\" >0</td>\n                        <td id=\"T_77656_row0_col6\" class=\"data row0 col6\" >0</td>\n                        <td id=\"T_77656_row0_col7\" class=\"data row0 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_77656_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n                        <td id=\"T_77656_row1_col0\" class=\"data row1 col0\" >000103f0d9cfb60f</td>\n                        <td id=\"T_77656_row1_col1\" class=\"data row1 col1\" >D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n                        <td id=\"T_77656_row1_col2\" class=\"data row1 col2\" >0</td>\n                        <td id=\"T_77656_row1_col3\" class=\"data row1 col3\" >0</td>\n                        <td id=\"T_77656_row1_col4\" class=\"data row1 col4\" >0</td>\n                        <td id=\"T_77656_row1_col5\" class=\"data row1 col5\" >0</td>\n                        <td id=\"T_77656_row1_col6\" class=\"data row1 col6\" >0</td>\n                        <td id=\"T_77656_row1_col7\" class=\"data row1 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_77656_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n                        <td id=\"T_77656_row2_col0\" class=\"data row2 col0\" >000113f07ec002fd</td>\n                        <td id=\"T_77656_row2_col1\" class=\"data row2 col1\" >Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n                        <td id=\"T_77656_row2_col2\" class=\"data row2 col2\" >0</td>\n                        <td id=\"T_77656_row2_col3\" class=\"data row2 col3\" >0</td>\n                        <td id=\"T_77656_row2_col4\" class=\"data row2 col4\" >0</td>\n                        <td id=\"T_77656_row2_col5\" class=\"data row2 col5\" >0</td>\n                        <td id=\"T_77656_row2_col6\" class=\"data row2 col6\" >0</td>\n                        <td id=\"T_77656_row2_col7\" class=\"data row2 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_77656_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n                        <td id=\"T_77656_row3_col0\" class=\"data row3 col0\" >0001b41b1c6bb37e</td>\n                        <td id=\"T_77656_row3_col1\" class=\"data row3 col1\" >\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of types of accidents  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport</td>\n                        <td id=\"T_77656_row3_col2\" class=\"data row3 col2\" >0</td>\n                        <td id=\"T_77656_row3_col3\" class=\"data row3 col3\" >0</td>\n                        <td id=\"T_77656_row3_col4\" class=\"data row3 col4\" >0</td>\n                        <td id=\"T_77656_row3_col5\" class=\"data row3 col5\" >0</td>\n                        <td id=\"T_77656_row3_col6\" class=\"data row3 col6\" >0</td>\n                        <td id=\"T_77656_row3_col7\" class=\"data row3 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_77656_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n                        <td id=\"T_77656_row4_col0\" class=\"data row4 col0\" >0001d958c54c6e35</td>\n                        <td id=\"T_77656_row4_col1\" class=\"data row4 col1\" >You, sir, are my hero. Any chance you remember what page that's on?</td>\n                        <td id=\"T_77656_row4_col2\" class=\"data row4 col2\" >0</td>\n                        <td id=\"T_77656_row4_col3\" class=\"data row4 col3\" >0</td>\n                        <td id=\"T_77656_row4_col4\" class=\"data row4 col4\" >0</td>\n                        <td id=\"T_77656_row4_col5\" class=\"data row4 col5\" >0</td>\n                        <td id=\"T_77656_row4_col6\" class=\"data row4 col6\" >0</td>\n                        <td id=\"T_77656_row4_col7\" class=\"data row4 col7\" >0</td>\n            </tr>\n    </tbody></table>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('./DATA/training.csv')\n",
    "test=pd.read_csv('./DATA/test.csv')\n",
    "train.head(5).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "# def remove_strip(str):\n",
    "#     return str.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "#\n",
    "# def get_stop_words(f, encoding='utf-8'):\n",
    "#     stop_words = []\n",
    "#     with open(f, \"r\", encoding=encoding) as f_stopwords:\n",
    "#         for line in f_stopwords:\n",
    "#             line = remove_strip(line)\n",
    "#             stop_words.append(line.lower())\n",
    "#     stop_words = set(stop_words)\n",
    "#     print(\"total counts: \", len(stop_words))\n",
    "#\n",
    "#     return stop_words\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# stop_words = get_stop_words(\"stopwords.txt\")\n",
    "# len(stop_words)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# train_text = train['comment_text'].apply(\n",
    "#     lambda text: \" \".join([w for w in text.replace(\",\", \"\").replace(\"?\", \"\").replace(\".\", \"\").replace(\"!\", \"\").split() if w.lower() not in stop_words]))\n",
    "# test_text = test['comment_text'].apply(\n",
    "#     lambda text: \" \".join([w for w in text.replace(\",\", \"\").replace(\"?\", \"\").replace(\".\", \"\").replace(\"!\", \"\").split() if w.lower() not in stop_words]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "all_text = pd.concat([train_text, test_text])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    min_df=30,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1,1),\n",
    "    max_features=20000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_tezhen = word_vectorizer.transform(train_text)\n",
    "test_word_tezhen = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    min_df=30,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=20000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_tezhen = char_vectorizer.transform(train_text)\n",
    "test_char_tezhen = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tezhen = hstack((train_word_tezhen, train_char_tezhen))\n",
    "test_tezhen = hstack((test_word_tezhen, test_char_tezhen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-7f50649b6b60>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature_selection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSelectKBest\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mkbest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSelectKBest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mtrain_selected\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkbest\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkbest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'toxic'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mtrain_selected\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "kbest = SelectKBest(k=10000)\n",
    "train_selected = kbest.fit(kbest, train['toxic'])\n",
    "train_selected\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_target = train['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tezhen, train_target, test_size=0.3, random_state=112)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic-训练集F1值:0.9136665501572876\n",
      "toxic-验证集F1值:0.7879541373117835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=112)\n",
    "classifier.fit(X_over, y_over)\n",
    "train_pred_toxic = classifier.predict(X_train)\n",
    "test_pred_toxic = classifier.predict(X_test)\n",
    "predict_toxic_0 = classifier.predict(test_tezhen)\n",
    "predict_toxic=pd.DataFrame(predict_toxic_0,columns=['toxic'])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print('toxic-训练集F1值:%s'%f1_score(train_pred_toxic,y_train))\n",
    "print('toxic-验证集F1值:%s'%f1_score(test_pred_toxic,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe_toxic-训练集F1值:0.724588781105019\n",
      "severe_toxic-验证集F1值:0.4995391705069124\n"
     ]
    }
   ],
   "source": [
    "train_target = train['severe_toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tezhen, train_target, test_size=0.3, random_state=112)#random113\n",
    "\n",
    "smote = SMOTE()\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=112)\n",
    "classifier.fit(X_over, y_over)\n",
    "train_pred_severe_toxic = classifier.predict(X_train)\n",
    "test_pred_severe_toxic = classifier.predict(X_test)\n",
    "\n",
    "predict_severe_toxic_0 = classifier.predict(test_tezhen)\n",
    "predict_severe_toxic=pd.DataFrame(predict_severe_toxic_0,columns=['severe_toxic'])\n",
    "\n",
    "print('severe_toxic-训练集F1值:%s'%f1_score(train_pred_severe_toxic,y_train))\n",
    "print('severe_toxic-验证集F1值:%s'%f1_score(test_pred_severe_toxic,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obscene-训练集F1值:0.9393939393939393\n",
      "obscene-验证集F1值:0.8204001952171791\n"
     ]
    }
   ],
   "source": [
    "train_target = train['obscene']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tezhen, train_target, test_size=0.3, random_state=112)\n",
    "\n",
    "smote = SMOTE()\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=112)\n",
    "classifier.fit(X_over, y_over)\n",
    "\n",
    "train_pred_obscene = classifier.predict(X_train)\n",
    "test_pred_obscene = classifier.predict(X_test)\n",
    "\n",
    "predict_obscene_0 = classifier.predict(test_tezhen)\n",
    "predict_obscene=pd.DataFrame(predict_obscene_0,columns=['obscene'])\n",
    "\n",
    "\n",
    "print('obscene-训练集F1值:%s'%f1_score(train_pred_obscene,y_train))\n",
    "print('obscene-验证集F1值:%s'%f1_score(test_pred_obscene,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threat-训练集F1值:0.8238897396630934\n",
      "threat-验证集F1值:0.5156794425087108\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_target = train['threat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tezhen, train_target, test_size=0.3, random_state=112)\n",
    "\n",
    "smote = SMOTE()\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=112)\n",
    "classifier.fit(X_over, y_over)\n",
    "train_pred_threat = classifier.predict(X_train)\n",
    "test_pred_threat = classifier.predict(X_test)\n",
    "\n",
    "predict_threat_0 = classifier.predict(test_tezhen)\n",
    "predict_threat=pd.DataFrame(predict_threat_0,columns=['threat'])\n",
    "\n",
    "\n",
    "print('threat-训练集F1值:%s'%f1_score(train_pred_threat,y_train))\n",
    "print('threat-验证集F1值:%s'%f1_score(test_pred_threat,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insult-训练集F1值:0.8882492032486893\n",
      "insult-验证集F1值:0.7275419545903258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_target = train['insult']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tezhen, train_target, test_size=0.3, random_state=112)\n",
    "\n",
    "smote = SMOTE()\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=112)\n",
    "classifier.fit(X_over, y_over)\n",
    "\n",
    "train_pred_insult = classifier.predict(X_train)\n",
    "test_pred_insult = classifier.predict(X_test)\n",
    "\n",
    "predict_insult_0 = classifier.predict(test_tezhen)\n",
    "predict_insult=pd.DataFrame(predict_insult_0,columns=['insult'])\n",
    "\n",
    "\n",
    "print('insult-训练集F1值:%s'%f1_score(train_pred_insult,y_train))\n",
    "print('insult-验证集F1值:%s'%f1_score(test_pred_insult,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity_hate-训练集F1值:0.8004083716181726\n",
      "identity_hate-验证集F1值:0.47152619589977224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_target = train['identity_hate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_tezhen, train_target, test_size=0.3, random_state=112)\n",
    "\n",
    "smote = SMOTE()\n",
    "X_over, y_over = smote.fit_resample(X_train, y_train)\n",
    "classifier = LogisticRegression(C=3.0, solver='sag', random_state=112)\n",
    "classifier.fit(X_over, y_over)\n",
    "\n",
    "train_pred_identity_hate = classifier.predict(X_train)\n",
    "test_pred_identity_hate = classifier.predict(X_test)\n",
    "\n",
    "predict_identity_hate_0 = classifier.predict(test_tezhen)\n",
    "predict_identity_hate=pd.DataFrame(predict_identity_hate_0,columns=['identity_hate'])\n",
    "\n",
    "\n",
    "print('identity_hate-训练集F1值:%s'%f1_score(train_pred_identity_hate,y_train))\n",
    "print('identity_hate-验证集F1值:%s'%f1_score(test_pred_identity_hate,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.concat([test[['id']],predict_toxic,\n",
    "                     predict_severe_toxic,predict_obscene,predict_threat,\n",
    "                     predict_insult,predict_identity_hate],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for column in final.columns[1:]:\n",
    "    final[column] = final[column].map({0:'H', 1:'L'})\n",
    "final.to_csv('方笠_results.csv', index=None)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-52a413d9",
   "language": "python",
   "display_name": "PyCharm (TwilightStruggle)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}