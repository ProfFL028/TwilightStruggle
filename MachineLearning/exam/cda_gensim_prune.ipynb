{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fc599509220>",
      "text/html": "<style  type=\"text/css\" >\n</style><table id=\"T_f72e3_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >id</th>        <th class=\"col_heading level0 col1\" >comment_text</th>        <th class=\"col_heading level0 col2\" >toxic</th>        <th class=\"col_heading level0 col3\" >severe_toxic</th>        <th class=\"col_heading level0 col4\" >obscene</th>        <th class=\"col_heading level0 col5\" >threat</th>        <th class=\"col_heading level0 col6\" >insult</th>        <th class=\"col_heading level0 col7\" >identity_hate</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_f72e3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n                        <td id=\"T_f72e3_row0_col0\" class=\"data row0 col0\" >0000997932d777bf</td>\n                        <td id=\"T_f72e3_row0_col1\" class=\"data row0 col1\" >Explanation\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n                        <td id=\"T_f72e3_row0_col2\" class=\"data row0 col2\" >0</td>\n                        <td id=\"T_f72e3_row0_col3\" class=\"data row0 col3\" >0</td>\n                        <td id=\"T_f72e3_row0_col4\" class=\"data row0 col4\" >0</td>\n                        <td id=\"T_f72e3_row0_col5\" class=\"data row0 col5\" >0</td>\n                        <td id=\"T_f72e3_row0_col6\" class=\"data row0 col6\" >0</td>\n                        <td id=\"T_f72e3_row0_col7\" class=\"data row0 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f72e3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n                        <td id=\"T_f72e3_row1_col0\" class=\"data row1 col0\" >000103f0d9cfb60f</td>\n                        <td id=\"T_f72e3_row1_col1\" class=\"data row1 col1\" >D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n                        <td id=\"T_f72e3_row1_col2\" class=\"data row1 col2\" >0</td>\n                        <td id=\"T_f72e3_row1_col3\" class=\"data row1 col3\" >0</td>\n                        <td id=\"T_f72e3_row1_col4\" class=\"data row1 col4\" >0</td>\n                        <td id=\"T_f72e3_row1_col5\" class=\"data row1 col5\" >0</td>\n                        <td id=\"T_f72e3_row1_col6\" class=\"data row1 col6\" >0</td>\n                        <td id=\"T_f72e3_row1_col7\" class=\"data row1 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f72e3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n                        <td id=\"T_f72e3_row2_col0\" class=\"data row2 col0\" >000113f07ec002fd</td>\n                        <td id=\"T_f72e3_row2_col1\" class=\"data row2 col1\" >Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n                        <td id=\"T_f72e3_row2_col2\" class=\"data row2 col2\" >0</td>\n                        <td id=\"T_f72e3_row2_col3\" class=\"data row2 col3\" >0</td>\n                        <td id=\"T_f72e3_row2_col4\" class=\"data row2 col4\" >0</td>\n                        <td id=\"T_f72e3_row2_col5\" class=\"data row2 col5\" >0</td>\n                        <td id=\"T_f72e3_row2_col6\" class=\"data row2 col6\" >0</td>\n                        <td id=\"T_f72e3_row2_col7\" class=\"data row2 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f72e3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n                        <td id=\"T_f72e3_row3_col0\" class=\"data row3 col0\" >0001b41b1c6bb37e</td>\n                        <td id=\"T_f72e3_row3_col1\" class=\"data row3 col1\" >\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of types of accidents  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport</td>\n                        <td id=\"T_f72e3_row3_col2\" class=\"data row3 col2\" >0</td>\n                        <td id=\"T_f72e3_row3_col3\" class=\"data row3 col3\" >0</td>\n                        <td id=\"T_f72e3_row3_col4\" class=\"data row3 col4\" >0</td>\n                        <td id=\"T_f72e3_row3_col5\" class=\"data row3 col5\" >0</td>\n                        <td id=\"T_f72e3_row3_col6\" class=\"data row3 col6\" >0</td>\n                        <td id=\"T_f72e3_row3_col7\" class=\"data row3 col7\" >0</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f72e3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n                        <td id=\"T_f72e3_row4_col0\" class=\"data row4 col0\" >0001d958c54c6e35</td>\n                        <td id=\"T_f72e3_row4_col1\" class=\"data row4 col1\" >You, sir, are my hero. Any chance you remember what page that's on?</td>\n                        <td id=\"T_f72e3_row4_col2\" class=\"data row4 col2\" >0</td>\n                        <td id=\"T_f72e3_row4_col3\" class=\"data row4 col3\" >0</td>\n                        <td id=\"T_f72e3_row4_col4\" class=\"data row4 col4\" >0</td>\n                        <td id=\"T_f72e3_row4_col5\" class=\"data row4 col5\" >0</td>\n                        <td id=\"T_f72e3_row4_col6\" class=\"data row4 col6\" >0</td>\n                        <td id=\"T_f72e3_row4_col7\" class=\"data row4 col7\" >0</td>\n            </tr>\n    </tbody></table>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"DATA/training.csv\")\n",
    "data.head(5).style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n       'identity_hate'],\n      dtype='object')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_columns = data.columns[2:]\n",
    "y_columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([-0.05419922,  0.01708984, -0.00527954,  0.33203125, -0.25      ,\n       -0.01397705, -0.15039062, -0.265625  ,  0.01647949,  0.3828125 ,\n       -0.03295898, -0.09716797, -0.16308594, -0.04443359,  0.00946045,\n        0.18457031,  0.03637695,  0.16601562,  0.36328125, -0.25585938,\n        0.375     ,  0.171875  ,  0.21386719, -0.19921875,  0.13085938,\n       -0.07275391, -0.02819824,  0.11621094,  0.15332031,  0.09082031,\n        0.06787109, -0.0300293 , -0.16894531, -0.20800781, -0.03710938,\n       -0.22753906,  0.26367188,  0.012146  ,  0.18359375,  0.31054688,\n       -0.10791016, -0.19140625,  0.21582031,  0.13183594, -0.03515625,\n        0.18554688, -0.30859375,  0.04785156, -0.10986328,  0.14355469,\n       -0.43554688, -0.0378418 ,  0.10839844,  0.140625  , -0.10595703,\n        0.26171875, -0.17089844,  0.39453125,  0.12597656, -0.27734375,\n       -0.28125   ,  0.14746094, -0.20996094,  0.02355957,  0.18457031,\n        0.00445557, -0.27929688, -0.03637695, -0.29296875,  0.19628906,\n        0.20703125,  0.2890625 , -0.20507812,  0.06787109, -0.43164062,\n       -0.10986328, -0.2578125 , -0.02331543,  0.11328125,  0.23144531,\n       -0.04418945,  0.10839844, -0.2890625 , -0.09521484, -0.10351562,\n       -0.0324707 ,  0.07763672, -0.13378906,  0.22949219,  0.06298828,\n        0.08349609,  0.02929688, -0.11474609,  0.00534058, -0.12988281,\n        0.02514648,  0.08789062,  0.24511719, -0.11474609, -0.296875  ,\n       -0.59375   , -0.29492188, -0.13378906,  0.27734375, -0.04174805,\n        0.11621094,  0.28320312,  0.00241089,  0.13867188, -0.00683594,\n       -0.30078125,  0.16210938,  0.01171875, -0.13867188,  0.48828125,\n        0.02880859,  0.02416992,  0.04736328,  0.05859375, -0.23828125,\n        0.02758789,  0.05981445, -0.03857422,  0.06933594,  0.14941406,\n       -0.10888672, -0.07324219,  0.08789062,  0.27148438,  0.06591797,\n       -0.37890625, -0.26171875, -0.13183594,  0.09570312, -0.3125    ,\n        0.10205078,  0.03063965,  0.23632812,  0.00582886,  0.27734375,\n        0.20507812, -0.17871094, -0.31445312, -0.01586914,  0.13964844,\n        0.13574219,  0.0390625 , -0.29296875,  0.234375  , -0.33984375,\n       -0.11816406,  0.10644531, -0.18457031, -0.02099609,  0.02563477,\n        0.25390625,  0.07275391,  0.13574219, -0.00138092, -0.2578125 ,\n       -0.2890625 ,  0.10107422,  0.19238281, -0.04882812,  0.27929688,\n       -0.3359375 , -0.07373047,  0.01879883, -0.10986328, -0.04614258,\n        0.15722656,  0.06689453, -0.03417969,  0.16308594,  0.08642578,\n        0.44726562,  0.02026367, -0.01977539,  0.07958984,  0.17773438,\n       -0.04370117, -0.00952148,  0.16503906,  0.17285156,  0.23144531,\n       -0.04272461,  0.02355957,  0.18359375, -0.41601562, -0.01745605,\n        0.16796875,  0.04736328,  0.14257812,  0.08496094,  0.33984375,\n        0.1484375 , -0.34375   , -0.14160156, -0.06835938, -0.14648438,\n       -0.02844238,  0.07421875, -0.07666016,  0.12695312,  0.05859375,\n       -0.07568359, -0.03344727,  0.23632812, -0.16308594,  0.16503906,\n        0.1484375 , -0.2421875 , -0.3515625 , -0.30664062,  0.00491333,\n        0.17675781,  0.46289062,  0.14257812, -0.25      , -0.25976562,\n        0.04370117,  0.34960938,  0.05957031,  0.07617188, -0.02868652,\n       -0.09667969, -0.01281738,  0.05859375, -0.22949219, -0.1953125 ,\n       -0.12207031,  0.20117188, -0.42382812,  0.06005859,  0.50390625,\n        0.20898438,  0.11230469, -0.06054688,  0.33203125,  0.07421875,\n       -0.05786133,  0.11083984, -0.06494141,  0.05639648,  0.01757812,\n        0.08398438,  0.13769531,  0.2578125 ,  0.16796875, -0.16894531,\n        0.01794434,  0.16015625,  0.26171875,  0.31640625, -0.24804688,\n        0.05371094, -0.0859375 ,  0.17089844, -0.39453125, -0.00156403,\n       -0.07324219, -0.04614258, -0.16210938, -0.15722656,  0.21289062,\n       -0.15820312,  0.04394531,  0.28515625,  0.01196289, -0.26953125,\n       -0.04370117,  0.37109375,  0.04663086, -0.19726562,  0.3046875 ,\n       -0.36523438, -0.23632812,  0.08056641, -0.04248047, -0.14648438,\n       -0.06225586, -0.0534668 , -0.05664062,  0.18945312,  0.37109375,\n       -0.22070312,  0.04638672,  0.02612305, -0.11474609,  0.265625  ,\n       -0.02453613,  0.11083984, -0.02514648, -0.12060547,  0.05297852,\n        0.07128906,  0.00063705, -0.36523438, -0.13769531, -0.12890625],\n      dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format(\n",
    "    fname=\"//Users/proffl/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "w2v['hello']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def remove_strip(str):\n",
    "    return str.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "\n",
    "def get_stop_words(f, encoding='utf-8'):\n",
    "    stop_words = []\n",
    "    with open(f, \"r\", encoding=encoding) as f_stopwords:\n",
    "        for line in f_stopwords:\n",
    "            line = remove_strip(line)\n",
    "            stop_words.append(line.lower())\n",
    "    stop_words = set(stop_words)\n",
    "    print(\"total counts: \", len(stop_words))\n",
    "\n",
    "    return stop_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total counts:  233\n"
     ]
    },
    {
     "data": {
      "text/plain": "233"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = get_stop_words(\"stopwords.txt\")\n",
    "len(stop_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0         Explanation edits made username Hardcore Metal...\n1         D'aww matches background colour seemingly stuc...\n2         man really trying edit war guy constantly remo...\n3         can't make real suggestions improvement wonder...\n4                      sir hero chance remember page that's\n                                ...                        \n127652    numbers parentheses additional decimal points ...\n127653       ashamed horrible thing put talk page 128611993\n127654    Spitzer Umm theres actual article prostitution...\n127655    looks like actually put speedy first version d...\n127656    really think understand came idea bad right aw...\nName: comment_text_sw, Length: 127657, dtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_text_sw'] = data['comment_text'].apply(\n",
    "    lambda text: \" \".join([w for w in text.replace(\",\", \"\").replace(\"?\", \"\").replace(\".\", \"\").replace(\"!\", \"\").split() if w.lower() not in stop_words]))\n",
    "data['comment_text_sw']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def sentence_to_word(sentence):\n",
    "    word_count = 0\n",
    "    sum_words = np.zeros(300)\n",
    "    for word in sentence.split():\n",
    "        word = word.strip()\n",
    "        if word in w2v.key_to_index:\n",
    "            sum_words = w2v[word] + sum_words\n",
    "            word_count += 1\n",
    "    if word_count > 0:\n",
    "        sum_words = sum_words / word_count\n",
    "    return sum_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6    \\\n0       0.016170  0.054041  0.008723  0.065613 -0.037635 -0.040747 -0.018361   \n1       0.013672  0.071279 -0.039398  0.040609 -0.057129  0.006200  0.075378   \n2       0.081226  0.062378 -0.008158  0.039550 -0.086282  0.070689  0.087277   \n3       0.012383  0.014619  0.036943  0.087166 -0.098782 -0.017439  0.054242   \n4       0.161540  0.058879  0.173024  0.180664 -0.045603 -0.006429  0.086690   \n...          ...       ...       ...       ...       ...       ...       ...   \n127652  0.044643  0.013181  0.071257  0.097162 -0.095079  0.007715  0.034559   \n127653  0.069867  0.088338  0.063371  0.054769 -0.027954  0.031840  0.154887   \n127654  0.043477 -0.024875 -0.086263  0.030362 -0.146620  0.105469 -0.007189   \n127655  0.017097  0.126302  0.057847  0.094198 -0.051073 -0.028998  0.099447   \n127656  0.044189  0.030304  0.031761  0.130107 -0.079898  0.009989  0.099881   \n\n             7         8         9    ...       290       291       292  \\\n0      -0.048341  0.084505  0.004627  ...  0.003094  0.046640 -0.100409   \n1      -0.118815  0.052134  0.049622  ... -0.154256  0.079244  0.010628   \n2      -0.042075  0.069681 -0.035172  ... -0.043018  0.014528 -0.065814   \n3      -0.042736  0.075439  0.024305  ... -0.051308  0.003052 -0.051037   \n4      -0.099447  0.104960  0.090093  ...  0.030334 -0.028900 -0.177775   \n...          ...       ...       ...  ...       ...       ...       ...   \n127652 -0.023131  0.098224  0.092019  ... -0.026332  0.072760 -0.082231   \n127653 -0.057861  0.190267  0.066772  ...  0.019246  0.121582 -0.150798   \n127654 -0.077033  0.037815  0.094177  ... -0.020698 -0.024414 -0.040473   \n127655 -0.005829  0.113953  0.076416  ... -0.041854  0.009006  0.011166   \n127656 -0.053382  0.040068  0.039925  ... -0.125814  0.120365 -0.138492   \n\n             293       294       295       296       297       298       299  \n0       0.064379  0.045702 -0.087487  0.027953 -0.106858 -0.008824 -0.032972  \n1       0.028564  0.075724 -0.153402 -0.078532  0.034414  0.070435  0.023519  \n2       0.019635 -0.080098 -0.048955  0.069958 -0.111234  0.003290  0.034442  \n3       0.020644  0.055660 -0.053549  0.034242 -0.125549 -0.001516 -0.011828  \n4      -0.012655 -0.027486 -0.120829 -0.041748 -0.121948 -0.008708  0.022003  \n...          ...       ...       ...       ...       ...       ...       ...  \n127652 -0.003951 -0.026439  0.048793  0.014121 -0.041830 -0.036802 -0.019561  \n127653  0.083577 -0.159383 -0.147664 -0.044474 -0.126322  0.042653 -0.054036  \n127654  0.080753 -0.048191  0.044569 -0.018324 -0.091404 -0.147135 -0.044403  \n127655  0.046495  0.065240 -0.024821  0.051866 -0.078274 -0.028307 -0.025798  \n127656  0.046231 -0.112892  0.034085  0.076057 -0.032998  0.072188  0.001112  \n\n[127657 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.016170</td>\n      <td>0.054041</td>\n      <td>0.008723</td>\n      <td>0.065613</td>\n      <td>-0.037635</td>\n      <td>-0.040747</td>\n      <td>-0.018361</td>\n      <td>-0.048341</td>\n      <td>0.084505</td>\n      <td>0.004627</td>\n      <td>...</td>\n      <td>0.003094</td>\n      <td>0.046640</td>\n      <td>-0.100409</td>\n      <td>0.064379</td>\n      <td>0.045702</td>\n      <td>-0.087487</td>\n      <td>0.027953</td>\n      <td>-0.106858</td>\n      <td>-0.008824</td>\n      <td>-0.032972</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.013672</td>\n      <td>0.071279</td>\n      <td>-0.039398</td>\n      <td>0.040609</td>\n      <td>-0.057129</td>\n      <td>0.006200</td>\n      <td>0.075378</td>\n      <td>-0.118815</td>\n      <td>0.052134</td>\n      <td>0.049622</td>\n      <td>...</td>\n      <td>-0.154256</td>\n      <td>0.079244</td>\n      <td>0.010628</td>\n      <td>0.028564</td>\n      <td>0.075724</td>\n      <td>-0.153402</td>\n      <td>-0.078532</td>\n      <td>0.034414</td>\n      <td>0.070435</td>\n      <td>0.023519</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.081226</td>\n      <td>0.062378</td>\n      <td>-0.008158</td>\n      <td>0.039550</td>\n      <td>-0.086282</td>\n      <td>0.070689</td>\n      <td>0.087277</td>\n      <td>-0.042075</td>\n      <td>0.069681</td>\n      <td>-0.035172</td>\n      <td>...</td>\n      <td>-0.043018</td>\n      <td>0.014528</td>\n      <td>-0.065814</td>\n      <td>0.019635</td>\n      <td>-0.080098</td>\n      <td>-0.048955</td>\n      <td>0.069958</td>\n      <td>-0.111234</td>\n      <td>0.003290</td>\n      <td>0.034442</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.012383</td>\n      <td>0.014619</td>\n      <td>0.036943</td>\n      <td>0.087166</td>\n      <td>-0.098782</td>\n      <td>-0.017439</td>\n      <td>0.054242</td>\n      <td>-0.042736</td>\n      <td>0.075439</td>\n      <td>0.024305</td>\n      <td>...</td>\n      <td>-0.051308</td>\n      <td>0.003052</td>\n      <td>-0.051037</td>\n      <td>0.020644</td>\n      <td>0.055660</td>\n      <td>-0.053549</td>\n      <td>0.034242</td>\n      <td>-0.125549</td>\n      <td>-0.001516</td>\n      <td>-0.011828</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.161540</td>\n      <td>0.058879</td>\n      <td>0.173024</td>\n      <td>0.180664</td>\n      <td>-0.045603</td>\n      <td>-0.006429</td>\n      <td>0.086690</td>\n      <td>-0.099447</td>\n      <td>0.104960</td>\n      <td>0.090093</td>\n      <td>...</td>\n      <td>0.030334</td>\n      <td>-0.028900</td>\n      <td>-0.177775</td>\n      <td>-0.012655</td>\n      <td>-0.027486</td>\n      <td>-0.120829</td>\n      <td>-0.041748</td>\n      <td>-0.121948</td>\n      <td>-0.008708</td>\n      <td>0.022003</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>127652</th>\n      <td>0.044643</td>\n      <td>0.013181</td>\n      <td>0.071257</td>\n      <td>0.097162</td>\n      <td>-0.095079</td>\n      <td>0.007715</td>\n      <td>0.034559</td>\n      <td>-0.023131</td>\n      <td>0.098224</td>\n      <td>0.092019</td>\n      <td>...</td>\n      <td>-0.026332</td>\n      <td>0.072760</td>\n      <td>-0.082231</td>\n      <td>-0.003951</td>\n      <td>-0.026439</td>\n      <td>0.048793</td>\n      <td>0.014121</td>\n      <td>-0.041830</td>\n      <td>-0.036802</td>\n      <td>-0.019561</td>\n    </tr>\n    <tr>\n      <th>127653</th>\n      <td>0.069867</td>\n      <td>0.088338</td>\n      <td>0.063371</td>\n      <td>0.054769</td>\n      <td>-0.027954</td>\n      <td>0.031840</td>\n      <td>0.154887</td>\n      <td>-0.057861</td>\n      <td>0.190267</td>\n      <td>0.066772</td>\n      <td>...</td>\n      <td>0.019246</td>\n      <td>0.121582</td>\n      <td>-0.150798</td>\n      <td>0.083577</td>\n      <td>-0.159383</td>\n      <td>-0.147664</td>\n      <td>-0.044474</td>\n      <td>-0.126322</td>\n      <td>0.042653</td>\n      <td>-0.054036</td>\n    </tr>\n    <tr>\n      <th>127654</th>\n      <td>0.043477</td>\n      <td>-0.024875</td>\n      <td>-0.086263</td>\n      <td>0.030362</td>\n      <td>-0.146620</td>\n      <td>0.105469</td>\n      <td>-0.007189</td>\n      <td>-0.077033</td>\n      <td>0.037815</td>\n      <td>0.094177</td>\n      <td>...</td>\n      <td>-0.020698</td>\n      <td>-0.024414</td>\n      <td>-0.040473</td>\n      <td>0.080753</td>\n      <td>-0.048191</td>\n      <td>0.044569</td>\n      <td>-0.018324</td>\n      <td>-0.091404</td>\n      <td>-0.147135</td>\n      <td>-0.044403</td>\n    </tr>\n    <tr>\n      <th>127655</th>\n      <td>0.017097</td>\n      <td>0.126302</td>\n      <td>0.057847</td>\n      <td>0.094198</td>\n      <td>-0.051073</td>\n      <td>-0.028998</td>\n      <td>0.099447</td>\n      <td>-0.005829</td>\n      <td>0.113953</td>\n      <td>0.076416</td>\n      <td>...</td>\n      <td>-0.041854</td>\n      <td>0.009006</td>\n      <td>0.011166</td>\n      <td>0.046495</td>\n      <td>0.065240</td>\n      <td>-0.024821</td>\n      <td>0.051866</td>\n      <td>-0.078274</td>\n      <td>-0.028307</td>\n      <td>-0.025798</td>\n    </tr>\n    <tr>\n      <th>127656</th>\n      <td>0.044189</td>\n      <td>0.030304</td>\n      <td>0.031761</td>\n      <td>0.130107</td>\n      <td>-0.079898</td>\n      <td>0.009989</td>\n      <td>0.099881</td>\n      <td>-0.053382</td>\n      <td>0.040068</td>\n      <td>0.039925</td>\n      <td>...</td>\n      <td>-0.125814</td>\n      <td>0.120365</td>\n      <td>-0.138492</td>\n      <td>0.046231</td>\n      <td>-0.112892</td>\n      <td>0.034085</td>\n      <td>0.076057</td>\n      <td>-0.032998</td>\n      <td>0.072188</td>\n      <td>0.001112</td>\n    </tr>\n  </tbody>\n</table>\n<p>127657 rows × 300 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_vec'] = data['comment_text_sw'].map(sentence_to_word)\n",
    "data_word2vec = pd.DataFrame(data['comment_vec'].tolist())\n",
    "data_word2vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# 保存词向量，方便使用\n",
    "data_word2vec.to_csv(\"./DATA/training_vec.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "             0         1         2         3         4         5         6    \\\n19342   0.099197  0.190674  0.021590  0.060811  0.054769 -0.088867  0.112274   \n26435   0.065458  0.008771  0.010422  0.056048 -0.129584  0.002936  0.017648   \n59148   0.023877  0.045043 -0.004852  0.046195 -0.070198  0.006410  0.060228   \n97325  -0.106543  0.110254 -0.123340 -0.025610  0.007349 -0.045947  0.142871   \n80515   0.028351  0.075399  0.135037  0.078857 -0.156494  0.045817  0.007365   \n...          ...       ...       ...       ...       ...       ...       ...   \n25971   0.014221 -0.010061 -0.071289 -0.080566 -0.030330 -0.061882 -0.034944   \n104007 -0.057014 -0.014132  0.028564  0.053908 -0.094220  0.040156  0.075801   \n92168   0.088862  0.048256  0.022404  0.081980 -0.089489 -0.014628  0.108927   \n33385  -0.011505  0.101471 -0.030304 -0.110107 -0.097168 -0.032501 -0.023895   \n16242   0.034437  0.010871  0.022879  0.099598 -0.058468 -0.048208  0.044659   \n\n             7         8         9    ...       290       291       292  \\\n19342  -0.119914  0.079346  0.064819  ... -0.109131  0.003337 -0.117767   \n26435  -0.060471  0.156436  0.071867  ... -0.045100  0.062325 -0.109375   \n59148  -0.021553  0.142289  0.062154  ... -0.050459  0.011416 -0.057102   \n97325   0.010791  0.080859 -0.023993  ... -0.080273 -0.105176 -0.013953   \n80515  -0.046319  0.078328 -0.018365  ...  0.058105  0.068387 -0.095318   \n...          ...       ...       ...  ...       ...       ...       ...   \n25971  -0.133537  0.065405  0.016570  ... -0.025293 -0.110262 -0.002504   \n104007 -0.021935  0.111272  0.001972  ... -0.103403  0.022513 -0.051532   \n92168  -0.050814  0.043604  0.027070  ... -0.079612  0.050866 -0.061425   \n33385  -0.033264  0.027924 -0.029709  ... -0.089859  0.006691 -0.189941   \n16242  -0.016695  0.140435 -0.037251  ... -0.000848  0.006090 -0.146393   \n\n             293       294       295       296       297       298       299  \n19342   0.041565 -0.050173  0.089539 -0.092163 -0.112183  0.138804  0.008586  \n26435  -0.024179  0.020745 -0.096464  0.027435 -0.033756  0.037328 -0.017438  \n59148   0.048687 -0.023334 -0.035901  0.023157 -0.095476  0.011501  0.008344  \n97325   0.042041  0.075513 -0.174963  0.111890 -0.002563 -0.073096 -0.068506  \n80515   0.011502 -0.031692 -0.114729  0.062995 -0.040409  0.002048 -0.011027  \n...          ...       ...       ...       ...       ...       ...       ...  \n25971   0.036800  0.024343 -0.057019 -0.068270  0.042765  0.077612 -0.099731  \n104007 -0.038753 -0.066249 -0.051870  0.013155 -0.139806 -0.123854  0.008334  \n92168   0.061433 -0.034974 -0.001198  0.035239 -0.048408  0.015377  0.023291  \n33385   0.053688 -0.080383 -0.145081  0.085739  0.140991  0.078552  0.026669  \n16242   0.036132  0.026352 -0.058487  0.035679 -0.083900 -0.011553 -0.019061  \n\n[85530 rows x 300 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>290</th>\n      <th>291</th>\n      <th>292</th>\n      <th>293</th>\n      <th>294</th>\n      <th>295</th>\n      <th>296</th>\n      <th>297</th>\n      <th>298</th>\n      <th>299</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19342</th>\n      <td>0.099197</td>\n      <td>0.190674</td>\n      <td>0.021590</td>\n      <td>0.060811</td>\n      <td>0.054769</td>\n      <td>-0.088867</td>\n      <td>0.112274</td>\n      <td>-0.119914</td>\n      <td>0.079346</td>\n      <td>0.064819</td>\n      <td>...</td>\n      <td>-0.109131</td>\n      <td>0.003337</td>\n      <td>-0.117767</td>\n      <td>0.041565</td>\n      <td>-0.050173</td>\n      <td>0.089539</td>\n      <td>-0.092163</td>\n      <td>-0.112183</td>\n      <td>0.138804</td>\n      <td>0.008586</td>\n    </tr>\n    <tr>\n      <th>26435</th>\n      <td>0.065458</td>\n      <td>0.008771</td>\n      <td>0.010422</td>\n      <td>0.056048</td>\n      <td>-0.129584</td>\n      <td>0.002936</td>\n      <td>0.017648</td>\n      <td>-0.060471</td>\n      <td>0.156436</td>\n      <td>0.071867</td>\n      <td>...</td>\n      <td>-0.045100</td>\n      <td>0.062325</td>\n      <td>-0.109375</td>\n      <td>-0.024179</td>\n      <td>0.020745</td>\n      <td>-0.096464</td>\n      <td>0.027435</td>\n      <td>-0.033756</td>\n      <td>0.037328</td>\n      <td>-0.017438</td>\n    </tr>\n    <tr>\n      <th>59148</th>\n      <td>0.023877</td>\n      <td>0.045043</td>\n      <td>-0.004852</td>\n      <td>0.046195</td>\n      <td>-0.070198</td>\n      <td>0.006410</td>\n      <td>0.060228</td>\n      <td>-0.021553</td>\n      <td>0.142289</td>\n      <td>0.062154</td>\n      <td>...</td>\n      <td>-0.050459</td>\n      <td>0.011416</td>\n      <td>-0.057102</td>\n      <td>0.048687</td>\n      <td>-0.023334</td>\n      <td>-0.035901</td>\n      <td>0.023157</td>\n      <td>-0.095476</td>\n      <td>0.011501</td>\n      <td>0.008344</td>\n    </tr>\n    <tr>\n      <th>97325</th>\n      <td>-0.106543</td>\n      <td>0.110254</td>\n      <td>-0.123340</td>\n      <td>-0.025610</td>\n      <td>0.007349</td>\n      <td>-0.045947</td>\n      <td>0.142871</td>\n      <td>0.010791</td>\n      <td>0.080859</td>\n      <td>-0.023993</td>\n      <td>...</td>\n      <td>-0.080273</td>\n      <td>-0.105176</td>\n      <td>-0.013953</td>\n      <td>0.042041</td>\n      <td>0.075513</td>\n      <td>-0.174963</td>\n      <td>0.111890</td>\n      <td>-0.002563</td>\n      <td>-0.073096</td>\n      <td>-0.068506</td>\n    </tr>\n    <tr>\n      <th>80515</th>\n      <td>0.028351</td>\n      <td>0.075399</td>\n      <td>0.135037</td>\n      <td>0.078857</td>\n      <td>-0.156494</td>\n      <td>0.045817</td>\n      <td>0.007365</td>\n      <td>-0.046319</td>\n      <td>0.078328</td>\n      <td>-0.018365</td>\n      <td>...</td>\n      <td>0.058105</td>\n      <td>0.068387</td>\n      <td>-0.095318</td>\n      <td>0.011502</td>\n      <td>-0.031692</td>\n      <td>-0.114729</td>\n      <td>0.062995</td>\n      <td>-0.040409</td>\n      <td>0.002048</td>\n      <td>-0.011027</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25971</th>\n      <td>0.014221</td>\n      <td>-0.010061</td>\n      <td>-0.071289</td>\n      <td>-0.080566</td>\n      <td>-0.030330</td>\n      <td>-0.061882</td>\n      <td>-0.034944</td>\n      <td>-0.133537</td>\n      <td>0.065405</td>\n      <td>0.016570</td>\n      <td>...</td>\n      <td>-0.025293</td>\n      <td>-0.110262</td>\n      <td>-0.002504</td>\n      <td>0.036800</td>\n      <td>0.024343</td>\n      <td>-0.057019</td>\n      <td>-0.068270</td>\n      <td>0.042765</td>\n      <td>0.077612</td>\n      <td>-0.099731</td>\n    </tr>\n    <tr>\n      <th>104007</th>\n      <td>-0.057014</td>\n      <td>-0.014132</td>\n      <td>0.028564</td>\n      <td>0.053908</td>\n      <td>-0.094220</td>\n      <td>0.040156</td>\n      <td>0.075801</td>\n      <td>-0.021935</td>\n      <td>0.111272</td>\n      <td>0.001972</td>\n      <td>...</td>\n      <td>-0.103403</td>\n      <td>0.022513</td>\n      <td>-0.051532</td>\n      <td>-0.038753</td>\n      <td>-0.066249</td>\n      <td>-0.051870</td>\n      <td>0.013155</td>\n      <td>-0.139806</td>\n      <td>-0.123854</td>\n      <td>0.008334</td>\n    </tr>\n    <tr>\n      <th>92168</th>\n      <td>0.088862</td>\n      <td>0.048256</td>\n      <td>0.022404</td>\n      <td>0.081980</td>\n      <td>-0.089489</td>\n      <td>-0.014628</td>\n      <td>0.108927</td>\n      <td>-0.050814</td>\n      <td>0.043604</td>\n      <td>0.027070</td>\n      <td>...</td>\n      <td>-0.079612</td>\n      <td>0.050866</td>\n      <td>-0.061425</td>\n      <td>0.061433</td>\n      <td>-0.034974</td>\n      <td>-0.001198</td>\n      <td>0.035239</td>\n      <td>-0.048408</td>\n      <td>0.015377</td>\n      <td>0.023291</td>\n    </tr>\n    <tr>\n      <th>33385</th>\n      <td>-0.011505</td>\n      <td>0.101471</td>\n      <td>-0.030304</td>\n      <td>-0.110107</td>\n      <td>-0.097168</td>\n      <td>-0.032501</td>\n      <td>-0.023895</td>\n      <td>-0.033264</td>\n      <td>0.027924</td>\n      <td>-0.029709</td>\n      <td>...</td>\n      <td>-0.089859</td>\n      <td>0.006691</td>\n      <td>-0.189941</td>\n      <td>0.053688</td>\n      <td>-0.080383</td>\n      <td>-0.145081</td>\n      <td>0.085739</td>\n      <td>0.140991</td>\n      <td>0.078552</td>\n      <td>0.026669</td>\n    </tr>\n    <tr>\n      <th>16242</th>\n      <td>0.034437</td>\n      <td>0.010871</td>\n      <td>0.022879</td>\n      <td>0.099598</td>\n      <td>-0.058468</td>\n      <td>-0.048208</td>\n      <td>0.044659</td>\n      <td>-0.016695</td>\n      <td>0.140435</td>\n      <td>-0.037251</td>\n      <td>...</td>\n      <td>-0.000848</td>\n      <td>0.006090</td>\n      <td>-0.146393</td>\n      <td>0.036132</td>\n      <td>0.026352</td>\n      <td>-0.058487</td>\n      <td>0.035679</td>\n      <td>-0.083900</td>\n      <td>-0.011553</td>\n      <td>-0.019061</td>\n    </tr>\n  </tbody>\n</table>\n<p>85530 rows × 300 columns</p>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "models = []\n",
    "i = 0\n",
    "test_size = 0.33\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    data_word2vec, data[y_columns[i]], test_size=test_size, stratify=data[y_columns[i]], random_state=random_seed)\n",
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(154736, 300)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rus = SMOTE()\n",
    "X, y= rus.fit_resample(X_train, y_train)\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set f1 score:  0.8877714538321269\n",
      "training set toxic f1_score: 65.88\n"
     ]
    }
   ],
   "source": [
    "lgb =LGBMClassifier(max_depth=10, num_leaves=400, boosting_type='dart', random_state=random_seed)\n",
    "lgb.fit(X, y)\n",
    "y_train_pred = lgb.predict(X_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "print(\"train set f1 score: \", f1_score(y_train, y_train_pred))\n",
    "print(\"training set %s f1_score: %.2f\" % (y_columns[i], f1_score(y_valid, y_valid_pred) * 100))\n",
    "models.append(lgb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.7585355820649938\n",
      "[[109549   5926]\n",
      " [  1118  11064]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 180x180 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_all_pred = lgb.predict(data_word2vec)\n",
    "confmt = confusion_matrix(y_true=data[y_columns[i]], y_pred=y_all_pred)\n",
    "print(\"f1_score:\", f1_score(y_all_pred, data[y_columns[i]]))\n",
    "print(confmt)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmt, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmt.shape[0]):\n",
    "    for j in range(confmt.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmt[i, j], va='center', ha='center')\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set f1 score:  0.9973780807551127\n",
      "training set severe_toxic f1_score: 93.99\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "rus = RandomUnderSampler(random_state=random_seed, )\n",
    "X, y= rus.fit_resample(data_word2vec, data[y_columns[i]])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "lgb =LGBMClassifier(max_depth=8, num_leaves=40, boosting_type='dart', random_state=random_seed)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_train_pred = lgb.predict(X_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "print(\"train set f1 score: \", f1_score(y_train, y_train_pred))\n",
    "print(\"training set %s f1_score: %.2f\" % (y_columns[i], f1_score(y_valid, y_valid_pred) * 100))\n",
    "models.append(lgb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set f1 score:  0.9900600334612736\n",
      "training set obscene f1_score: 89.13\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "rus = RandomUnderSampler(random_state=random_seed, )\n",
    "X, y= rus.fit_resample(data_word2vec, data[y_columns[i]])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "lgb =LGBMClassifier(max_depth=10, num_leaves=80, boosting_type='dart', random_state=random_seed)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_train_pred = lgb.predict(X_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "print(\"train set f1 score: \", f1_score(y_train, y_train_pred))\n",
    "print(\"training set %s f1_score: %.2f\" % (y_columns[i], f1_score(y_valid, y_valid_pred) * 100))\n",
    "models.append(lgb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set f1 score:  1.0\n",
      "training set threat f1_score: 92.71\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "rus = RandomUnderSampler(random_state=random_seed, )\n",
    "X, y= rus.fit_resample(data_word2vec, data[y_columns[i]])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "lgb =LGBMClassifier(max_depth=6, num_leaves=40, boosting_type='dart', random_state=random_seed)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_train_pred = lgb.predict(X_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "print(\"train set f1 score: \", f1_score(y_train, y_train_pred))\n",
    "print(\"training set %s f1_score: %.2f\" % (y_columns[i], f1_score(y_valid, y_valid_pred) * 100))\n",
    "models.append(lgb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set f1 score:  0.9616724738675957\n",
      "training set insult f1_score: 88.33\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "rus = RandomUnderSampler(random_state=random_seed, )\n",
    "X, y= rus.fit_resample(data_word2vec, data[y_columns[i]])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "lgb =LGBMClassifier(max_depth=7, num_leaves=40, boosting_type='dart', random_state=random_seed)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_train_pred = lgb.predict(X_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "print(\"train set f1 score: \", f1_score(y_train, y_train_pred))\n",
    "print(\"training set %s f1_score: %.2f\" % (y_columns[i], f1_score(y_valid, y_valid_pred) * 100))\n",
    "models.append(lgb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set f1 score:  0.9970605526161082\n",
      "training set identity_hate f1_score: 88.07\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "rus = RandomUnderSampler(random_state=random_seed, )\n",
    "X, y= rus.fit_resample(data_word2vec, data[y_columns[i]])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_seed)\n",
    "lgb =LGBMClassifier(max_depth=7, num_leaves=40, boosting_type='dart', random_state=random_seed)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_train_pred = lgb.predict(X_train)\n",
    "y_valid_pred = lgb.predict(X_valid)\n",
    "print(\"train set f1 score: \", f1_score(y_train, y_train_pred))\n",
    "print(\"training set %s f1_score: %.2f\" % (y_columns[i], f1_score(y_valid, y_valid_pred) * 100))\n",
    "models.append(lgb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.14248657778850213\n",
      "[[112261  14121]\n",
      " [    94   1181]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 180x180 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_all_pred = lgb.predict(data_word2vec)\n",
    "confmt = confusion_matrix(y_true=data[y_columns[i]], y_pred=y_all_pred)\n",
    "print(\"f1_score:\", f1_score(y_all_pred, data[y_columns[i]]))\n",
    "print(confmt)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "ax.matshow(confmt, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(confmt.shape[0]):\n",
    "    for j in range(confmt.shape[1]):\n",
    "        ax.text(x=j, y=i, s=confmt[i, j], va='center', ha='center')\n",
    "plt.xlabel(\"predicted label\")\n",
    "plt.ylabel(\"true label\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"./DATA/test.csv\", encoding='ISO8859-2')\n",
    "test['comment_text_sw'] = test['comment_text'].apply(\n",
    "    lambda text: \" \".join([w for w in text.replace(\",\", \"\").replace(\"?\", \"\").replace(\".\", \"\").replace(\"!\", \"\").split() if w.lower() not in stop_words]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "test['comment_vec'] = test['comment_text_sw'].map(sentence_to_word)\n",
    "test_word2vec = pd.DataFrame(test['comment_vec'].tolist())\n",
    "result = test['id']\n",
    "for (idx, column) in enumerate(y_columns):\n",
    "    y_test = models[idx].predict(test_word2vec)\n",
    "    result = pd.concat([result, pd.DataFrame(y_test, columns=[column], index=test.index)], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# for y in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']:\n",
    "#     result[y] = result[y].map({0:'L', 1:'H'})\n",
    "result.to_csv(\"方笠_results.csv\", index=None)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}